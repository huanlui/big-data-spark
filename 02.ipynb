{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'warrior'),\n",
       " (1, 'wizard'),\n",
       " (2, 'wizard'),\n",
       " (3, 'priest'),\n",
       " (4, 'warrior'),\n",
       " (5, 'warrior'),\n",
       " (6, 'warrior'),\n",
       " (7, 'priest'),\n",
       " (8, 'warrior'),\n",
       " (9, 'wizard'),\n",
       " (10, 'priest'),\n",
       " (11, 'wizard')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(23)\n",
    "ids = range(12)\n",
    "data = [ (id_,random.choice(['wizard','warrior','priest'])) for id_ in ids]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto sería para crear un RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los RDD son más lentos en Python porque no es tipado. Eso no pasa en escala. Además, nosotros querremos trabajar con _tablas_, no con _listas_ de datos. Un RDD tendría sentido por ejemplo si quiero trabajar con una lista  de logs, por ejemplo, pero para datos con varias propiedades, deberíamos usar dataframes, porque son mcuho más fáciles y naturales de usar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí los nombres los está poniendo a capón, está infiriendo el esquema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos elegirlo nosotros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, category: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(data, schema=['id', 'category'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por debajo, los dataframes usan RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[13] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, category='warrior'),\n",
       " Row(id=1, category='wizard'),\n",
       " Row(id=2, category='wizard'),\n",
       " Row(id=3, category='priest'),\n",
       " Row(id=4, category='warrior')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=0, category='warrior')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onerow = df.first()\n",
    "onerow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se recomienda hacer lo siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'warrior'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onerow.category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recomiendo mejor hacer esto, porque hay nombres complejos. No sé si estoy de acuerdo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'warrior'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onerow['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos especificar el esquema con más detalle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando nosotros estamos usando pyspark, en el fondo acabamos llamando a Java (Scala está montada sobre Java). Py4Java permite llamar a código Java usando python y eso es lo que usa pyspark por debajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,IntegerType,false),StructField(category,StringType,true)))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema_type = types.StructType([\n",
    "    types.StructField('id', types.IntegerType(), nullable = False),\n",
    "    types.StructField('category', types.StringType(), nullable = True),\n",
    "])\n",
    "\n",
    "schema_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data,schema=schema_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leyendo desde csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: bigint, _c1: int, _c2: string, _c3: string, _c4: string, _c5: string, _c6: double, _c7: string, _c8: int, _c9: string, _c10: string, _c11: string, _c12: int, _c13: string, _c14: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv('data/coupon150720.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM csv.`data/coupon150720.csv`')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto es util para gente que no tiene mucha idea de programar pero si de SQL o para directamente filtrar datos que no me interesan ya durantre el proceso de lectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79062005698500,1,MAA,AUH,9W,9W,56.79,USD,1,H,H,0526,150904,OK,IAF0\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 data/coupon150720.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[tft_number: string, coupon_number: string, origin: string, amount: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''SELECT  \n",
    "            _c0 AS tft_number,\n",
    "            _c1 AS coupon_number,\n",
    "            _c2 AS origin,\n",
    "            _c3 AS amount\n",
    "            FROM csv.`data/coupon150720.csv`''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones sobre dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  0| warrior|\n",
      "|  1|  wizard|\n",
      "|  2|  wizard|\n",
      "|  3|  priest|\n",
      "|  4| warrior|\n",
      "|  5| warrior|\n",
      "|  6| warrior|\n",
      "|  7|  priest|\n",
      "|  8| warrior|\n",
      "|  9|  wizard|\n",
      "| 10|  priest|\n",
      "| 11|  wizard|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|category|\n",
      "+--------+\n",
      "| warrior|\n",
      "|  wizard|\n",
      "|  wizard|\n",
      "|  priest|\n",
      "| warrior|\n",
      "| warrior|\n",
      "| warrior|\n",
      "|  priest|\n",
      "| warrior|\n",
      "|  wizard|\n",
      "|  priest|\n",
      "|  wizard|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('category').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[category: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'category'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas dos operaciones no son equivalente. De hecho, no puedo hacer por ejemplo el .show(). Es una referencia a la columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['category'].show() # cascaría"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  6| warrior|\n",
      "|  7|  priest|\n",
      "|  8| warrior|\n",
      "|  9|  wizard|\n",
      "| 10|  priest|\n",
      "| 11|  wizard|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['id'] > 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Where` hace lo mismo, pero el nombre es más _funcional_. De hecho, aquí me dice que lo encapsula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method filter of DataFrame[id: int, category: string]>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio: extraer ids que correspondan a priest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  3|\n",
      "|  7|\n",
      "| 10|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    ".where(df['category'] == 'priest') \\\n",
    ".select('id') \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción para usar select es usar el doble corchete. Recuerda que el corchete simple es solo para meterlo en filtros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  3|\n",
      "|  7|\n",
      "| 10|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    ".where(df['category'] == 'priest') \\\n",
    "[['id']] \\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los dataframe son inmutables. Por tanto, no puedo hacer cosas como esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['new_column'] = df['id']  * 100 (casca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo correcto sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, category: string, (id * 100): int]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('id', 'category', df['id'] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+\n",
      "| id|category|new_column|\n",
      "+---+--------+----------+\n",
      "|  0| warrior|         0|\n",
      "|  1|  wizard|       100|\n",
      "|  2|  wizard|       200|\n",
      "|  3|  priest|       300|\n",
      "|  4| warrior|       400|\n",
      "|  5| warrior|       500|\n",
      "|  6| warrior|       600|\n",
      "|  7|  priest|       700|\n",
      "|  8| warrior|       800|\n",
      "|  9|  wizard|       900|\n",
      "| 10|  priest|      1000|\n",
      "| 11|  wizard|      1100|\n",
      "+---+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('new_column',df['id'] * 100).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTA__: Ojo, todo esto no está ocupando memoria. Por ejemplo, cuando he creado la dataframe a partir del csv, lo que se almacena es que estamos haciendo referencia a un  path de csv. Es a la hora de hacer un take o similar cuando realmente lee de ese fichero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer transformaciones en columnas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log hace el logaritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|           LOG(id)|\n",
      "+------------------+\n",
      "|              null|\n",
      "|               0.0|\n",
      "|0.6931471805599453|\n",
      "|1.0986122886681098|\n",
      "|1.3862943611198906|\n",
      "|1.6094379124341003|\n",
      "| 1.791759469228055|\n",
      "|1.9459101490553132|\n",
      "|2.0794415416798357|\n",
      "|2.1972245773362196|\n",
      "| 2.302585092994046|\n",
      "|2.3978952727983707|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[[functions.log('id')]].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|           LOG(id)|\n",
      "+------------------+\n",
      "|              null|\n",
      "|               0.0|\n",
      "|0.6931471805599453|\n",
      "|1.0986122886681098|\n",
      "|1.3862943611198906|\n",
      "|1.6094379124341003|\n",
      "| 1.791759469228055|\n",
      "|1.9459101490553132|\n",
      "|2.0794415416798357|\n",
      "|2.1972245773362196|\n",
      "| 2.302585092994046|\n",
      "|2.3978952727983707|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(functions.log('id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero quiero hacerme `mi propia función`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|<lambda>(id)|\n",
      "+---+------------+\n",
      "|  0|           0|\n",
      "|  1|           1|\n",
      "|  2|           4|\n",
      "|  3|           9|\n",
      "|  4|          16|\n",
      "|  5|          25|\n",
      "|  6|          36|\n",
      "|  7|          49|\n",
      "|  8|          64|\n",
      "|  9|          81|\n",
      "| 10|         100|\n",
      "| 11|         121|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mi_func = lambda x :  x * x\n",
    "mi_udf = functions.udf(mi_func)\n",
    "otro_df = df.select('id', mi_udf('id'))\n",
    "otro_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, <lambda>(id): string]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otro_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo jodido es que me pone la columna de tipo string el muy c... Esto es porque spark lo intenta inferir, perop python no tiene tipado estático. Spark tira por string \n",
    "\n",
    "```Esto con Scala no pasaba```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es tan dramático, lo puedo solucionar fácil. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, <lambda>(id): int]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi_func = lambda x :  x * x\n",
    "mi_udf = functions.udf(mi_func, returnType=types.IntegerType())\n",
    "df.select('id', mi_udf('id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear una columna puntos de vida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------------------------+\n",
      "| id|category|get_puntos_de_vida(category)|\n",
      "+---+--------+----------------------------+\n",
      "|  0| warrior|                          50|\n",
      "|  1|  wizard|                          32|\n",
      "|  2|  wizard|                          33|\n",
      "|  3|  priest|                          14|\n",
      "|  4| warrior|                          54|\n",
      "|  5| warrior|                          50|\n",
      "|  6| warrior|                          54|\n",
      "|  7|  priest|                          14|\n",
      "|  8| warrior|                          54|\n",
      "|  9|  wizard|                          30|\n",
      "| 10|  priest|                          12|\n",
      "| 11|  wizard|                          31|\n",
      "+---+--------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_puntos_de_vida(category):\n",
    "    random_deviation = round(random.random() * 5)\n",
    "    \n",
    "    if(category == 'priest'): return 10 + random_deviation\n",
    "    if(category == 'warrior'): return 50 + random_deviation\n",
    "    \n",
    "    return 30 + random_deviation\n",
    "\n",
    "df.select('id', 'category', functions.udf(get_puntos_de_vida)('category')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más elegante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+\n",
      "| id|category| hp|\n",
      "+---+--------+---+\n",
      "|  0| warrior| 50|\n",
      "|  1|  wizard| 30|\n",
      "|  2|  wizard| 30|\n",
      "|  3|  priest| 10|\n",
      "|  4| warrior| 50|\n",
      "|  5| warrior| 50|\n",
      "|  6| warrior| 50|\n",
      "|  7|  priest| 10|\n",
      "|  8| warrior| 50|\n",
      "|  9|  wizard| 30|\n",
      "| 10|  priest| 10|\n",
      "| 11|  wizard| 30|\n",
      "+---+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hp = functions.udf(lambda category: {'priest': 10, 'warrior': 50, 'wizard':30}[category])\n",
    "\n",
    "df.select('id', 'category', hp('category').cast(types.IntegerType()).alias('hp')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O incluso más bonito:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+\n",
      "| id|category|puntos de vida|\n",
      "+---+--------+--------------+\n",
      "|  0| warrior|            50|\n",
      "|  1|  wizard|            30|\n",
      "|  2|  wizard|            30|\n",
      "|  3|  priest|            10|\n",
      "|  4| warrior|            50|\n",
      "|  5| warrior|            50|\n",
      "|  6| warrior|            50|\n",
      "|  7|  priest|            10|\n",
      "|  8| warrior|            50|\n",
      "|  9|  wizard|            30|\n",
      "| 10|  priest|            10|\n",
      "| 11|  wizard|            30|\n",
      "+---+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn('puntos de vida', hp('category').cast(types.IntegerType()))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estadísticas de summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.dataframe.DataFrameStatFunctions at 0x1164ec590>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Concepto__: Bloom filter: algoritmo que te permite calcular al vuelo (sin guardarte en memoria nada) la pertenecia a conjuntos de forma aproximada.  Da falsos positivos pero no da falsos negativos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.24161209862606872"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.stat.corr('id', 'puntos de vida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14.545454545454545"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.cov('id','puntos de vida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+------+\n",
      "| id|category|puntos de vida|  land|\n",
      "+---+--------+--------------+------+\n",
      "|  0| warrior|            50|mordor|\n",
      "|  1|  wizard|            30|gondor|\n",
      "|  2|  wizard|            30|gondor|\n",
      "|  3|  priest|            10|mordor|\n",
      "|  4| warrior|            50|mordor|\n",
      "|  5| warrior|            50|gondor|\n",
      "|  6| warrior|            50|mordor|\n",
      "|  7|  priest|            10|mordor|\n",
      "|  8| warrior|            50|gondor|\n",
      "|  9|  wizard|            30|mordor|\n",
      "| 10|  priest|            10|mordor|\n",
      "| 11|  wizard|            30|gondor|\n",
      "+---+--------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "location_udf = functions.udf(lambda: random.choice(['gondor','mordor']))\n",
    "\n",
    "df3 = df2.withColumn('land', location_udf())\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`crosstab` genera la tabla de contigencia para dos columnas, que no es más que lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------+\n",
      "|category_land|gondor|mordor|\n",
      "+-------------+------+------+\n",
      "|       priest|     2|     1|\n",
      "|      warrior|     1|     4|\n",
      "|       wizard|     3|     1|\n",
      "+-------------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.crosstab('category', 'land').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para abrir consola de spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://juans-mbp:4040'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.uiWebUrl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+------+\n",
      "| id|category|puntos de vida|  land|\n",
      "+---+--------+--------------+------+\n",
      "|  0| warrior|            50|gondor|\n",
      "|  1|  wizard|            30|gondor|\n",
      "|  2|  wizard|            30|mordor|\n",
      "|  3|  priest|            10|gondor|\n",
      "|  4| warrior|            50|gondor|\n",
      "|  5| warrior|            50|mordor|\n",
      "|  6| warrior|            50|mordor|\n",
      "|  7|  priest|            10|mordor|\n",
      "|  8| warrior|            50|gondor|\n",
      "|  9|  wizard|            30|gondor|\n",
      "| 10|  priest|            10|mordor|\n",
      "| 11|  wizard|            30|gondor|\n",
      "+---+--------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df3.groupby('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|category|max(id)|\n",
      "+--------+-------+\n",
      "|  priest|     10|\n",
      "| warrior|      8|\n",
      "|  wizard|     11|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groups.max('id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer varias agregaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------+\n",
      "|category|avg(puntos de vida)|max(id)|\n",
      "+--------+-------------------+-------+\n",
      "|  priest|               10.0|     10|\n",
      "| warrior|               50.0|      8|\n",
      "|  wizard|               30.0|     11|\n",
      "+--------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groups.agg({'id': 'max', 'puntos de vida': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más flexible y bonita:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------------------+-----------------+\n",
      "|category|max(id)|avg(puntos de vida)|          avg(id)|\n",
      "+--------+-------+-------------------+-----------------+\n",
      "|  priest|     10|               10.0|6.666666666666667|\n",
      "| warrior|      8|               50.0|              4.6|\n",
      "|  wizard|     11|               30.0|             5.75|\n",
      "+--------+-------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groups.agg(functions.max('id'), functions.mean('puntos de vida'), functions.mean('id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También puedo agrupar usando expresiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|(id > 5)|avg(puntos de vida)|\n",
      "+--------+-------------------+\n",
      "|    true|               30.0|\n",
      "|   false| 36.666666666666664|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.groupBy(df['id'] > 5).mean('puntos de vida').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(19, 'gondor', 42000),\n",
       " (0, 'mordor', 37000),\n",
       " (8, 'gondor', 23000),\n",
       " (6, 'gondor', 16000),\n",
       " (22, 'mordor', 48000),\n",
       " (20, 'mordor', 23000),\n",
       " (26, 'gondor', 13000),\n",
       " (2, 'mordor', 13000),\n",
       " (12, 'mordor', 43000),\n",
       " (0, 'gondor', 34000)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import choices\n",
    "seed(42)\n",
    "\n",
    "data = list(zip(\n",
    "    choices(range(30),k=10),\n",
    "    choices(['gondor', 'mordor'], k=10),\n",
    "    choices(range(10000,50000, 1000), k=10)\n",
    "))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+\n",
      "| id|  land|hp_bonus|\n",
      "+---+------+--------+\n",
      "| 19|gondor|   42000|\n",
      "|  0|mordor|   37000|\n",
      "|  8|gondor|   23000|\n",
      "|  6|gondor|   16000|\n",
      "| 22|mordor|   48000|\n",
      "| 20|mordor|   23000|\n",
      "| 26|gondor|   13000|\n",
      "|  2|mordor|   13000|\n",
      "| 12|mordor|   43000|\n",
      "|  0|gondor|   34000|\n",
      "+---+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "right = spark.createDataFrame(data, schema=['id', 'land', 'hp_bonus'])\n",
    "right.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto siguiente casca. Primero porque va a usar dos columnas, y segundo porque tenemos ids repetidos, detecta un prpducto cartesiano y casca porque en big data un producto cartesiano es una big shit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.join(right).show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+------+------+--------+\n",
      "| id|category|puntos de vida|  land|  land|hp_bonus|\n",
      "+---+--------+--------------+------+------+--------+\n",
      "| 26|    null|          null|  null|gondor|   13000|\n",
      "| 19|    null|          null|  null|gondor|   42000|\n",
      "|  0| warrior|            50|mordor|mordor|   37000|\n",
      "|  0| warrior|            50|mordor|gondor|   34000|\n",
      "| 22|    null|          null|  null|mordor|   48000|\n",
      "|  7|  priest|            10|gondor|  null|    null|\n",
      "|  6| warrior|            50|mordor|gondor|   16000|\n",
      "|  9|  wizard|            30|mordor|  null|    null|\n",
      "|  5| warrior|            50|mordor|  null|    null|\n",
      "|  1|  wizard|            30|gondor|  null|    null|\n",
      "| 10|  priest|            10|gondor|  null|    null|\n",
      "|  3|  priest|            10|mordor|  null|    null|\n",
      "| 12|    null|          null|  null|mordor|   43000|\n",
      "|  8| warrior|            50|mordor|gondor|   23000|\n",
      "| 11|  wizard|            30|gondor|  null|    null|\n",
      "|  2|  wizard|            30|mordor|mordor|   13000|\n",
      "|  4| warrior|            50|gondor|  null|    null|\n",
      "| 20|    null|          null|  null|mordor|   23000|\n",
      "+---+--------+--------------+------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined = df3.join(right, on='id', how='outer')\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto nos da dos columnas diferentes para land, una de una tabla y otra de la otra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para especificar a cual me refiero, tengo que especificar el dataframe del que quiero sacar la columna land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  land|\n",
      "+------+\n",
      "|  null|\n",
      "|  null|\n",
      "|gondor|\n",
      "|gondor|\n",
      "|  null|\n",
      "|gondor|\n",
      "|gondor|\n",
      "|gondor|\n",
      "|gondor|\n",
      "|gondor|\n",
      "|mordor|\n",
      "|gondor|\n",
      "|  null|\n",
      "|mordor|\n",
      "|gondor|\n",
      "|mordor|\n",
      "|gondor|\n",
      "|  null|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined.select(df3['land']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  land|\n",
      "+------+\n",
      "|gondor|\n",
      "|gondor|\n",
      "|mordor|\n",
      "|gondor|\n",
      "|mordor|\n",
      "|  null|\n",
      "|gondor|\n",
      "|  null|\n",
      "|  null|\n",
      "|  null|\n",
      "|  null|\n",
      "|  null|\n",
      "|mordor|\n",
      "|gondor|\n",
      "|  null|\n",
      "|mordor|\n",
      "|  null|\n",
      "|mordor|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined.select(right['land']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cacheo `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para cachear por ejemplo este cache, usariamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, category: string, puntos de vida: int, land: string, land: string, hp_bonus: bigint]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero esto, si nos vamos a la seccion vemos que no nos aparece en el storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcular la z score de puntos de vida de cada jugador para su tierra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------+------+\n",
      "| id|category|puntos de vida|  land|\n",
      "+---+--------+--------------+------+\n",
      "|  0| warrior|            50|mordor|\n",
      "|  1|  wizard|            30|mordor|\n",
      "|  2|  wizard|            30|mordor|\n",
      "|  3|  priest|            10|mordor|\n",
      "|  4| warrior|            50|mordor|\n",
      "|  5| warrior|            50|mordor|\n",
      "|  6| warrior|            50|gondor|\n",
      "|  7|  priest|            10|gondor|\n",
      "|  8| warrior|            50|mordor|\n",
      "|  9|  wizard|            30|gondor|\n",
      "| 10|  priest|            10|gondor|\n",
      "| 11|  wizard|            30|gondor|\n",
      "+---+--------+--------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+------------------+\n",
      "|  land|               avg|            stddev|\n",
      "+------+------------------+------------------+\n",
      "|gondor|              42.0|10.954451150103322|\n",
      "|mordor|27.142857142857142|17.994708216848746|\n",
      "+------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "medias = df3.groupby('land') \\\n",
    ".agg(functions.avg('puntos de vida').alias('avg'), functions.stddev('puntos de vida').alias('stddev')) \n",
    "medias.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------+--------------+------------------+------------------+\n",
      "|  land| id|category|puntos de vida|               avg|            stddev|\n",
      "+------+---+--------+--------------+------------------+------------------+\n",
      "|gondor|  2|  wizard|            30|41.111111111111114|10.540925533894598|\n",
      "|gondor|  4| warrior|            50|41.111111111111114|10.540925533894598|\n",
      "|gondor|  6| warrior|            50|41.111111111111114|10.540925533894598|\n",
      "|gondor|  8| warrior|            50|41.111111111111114|10.540925533894598|\n",
      "|gondor| 11|  wizard|            30|41.111111111111114|10.540925533894598|\n",
      "|mordor|  0| warrior|            50|              10.0|               0.0|\n",
      "|mordor|  1|  wizard|            30|              10.0|               0.0|\n",
      "|mordor|  3|  priest|            10|              10.0|               0.0|\n",
      "|mordor|  5| warrior|            50|              10.0|               0.0|\n",
      "|mordor|  7|  priest|            10|              10.0|               0.0|\n",
      "|mordor|  9|  wizard|            30|              10.0|               0.0|\n",
      "|mordor| 10|  priest|            10|              10.0|               0.0|\n",
      "+------+---+--------+--------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annotated = df3.join(medias,on='land')\n",
    "annotated.cache() # si no cacheo, cambian los valores, porque esto es en streaming y tengo randoms por ahi. \n",
    "annotated.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora calculamos ya los z . Los podemos usar con aritmetica de columnas o con lambda. Lo primero es más eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------+--------------+------------------+------------------+------------------+\n",
      "|  land| id|category|puntos de vida|               avg|            stddev|           z-score|\n",
      "+------+---+--------+--------------+------------------+------------------+------------------+\n",
      "|gondor|  2|  wizard|            30|41.111111111111114|10.540925533894598| -1.05409255338946|\n",
      "|gondor|  4| warrior|            50|41.111111111111114|10.540925533894598|0.8432740427115676|\n",
      "|gondor|  6| warrior|            50|41.111111111111114|10.540925533894598|0.8432740427115676|\n",
      "|gondor|  8| warrior|            50|41.111111111111114|10.540925533894598|0.8432740427115676|\n",
      "|gondor| 11|  wizard|            30|41.111111111111114|10.540925533894598| -1.05409255338946|\n",
      "|mordor|  0| warrior|            50|              10.0|               0.0|              null|\n",
      "|mordor|  1|  wizard|            30|              10.0|               0.0|              null|\n",
      "|mordor|  3|  priest|            10|              10.0|               0.0|              null|\n",
      "|mordor|  5| warrior|            50|              10.0|               0.0|              null|\n",
      "|mordor|  7|  priest|            10|              10.0|               0.0|              null|\n",
      "|mordor|  9|  wizard|            30|              10.0|               0.0|              null|\n",
      "|mordor| 10|  priest|            10|              10.0|               0.0|              null|\n",
      "+------+---+--------+--------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = annotated.withColumn('z-score', (annotated['puntos de vida'] - annotated['avg']) / annotated['stddev'] )\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma (recordemos, algo más lenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore = functions.udf(lambda xi, average, std: (xi - average) / std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o381.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 44 in stage 208.0 failed 1 times, most recent failure: Lost task 44.0 in stage 208.0 (TID 2780, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-63-55ec9c8cbe08>\", line 1, in <lambda>\nZeroDivisionError: float division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor135.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-63-55ec9c8cbe08>\", line 1, in <lambda>\nZeroDivisionError: float division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-fd84af91d880>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresult2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'z-score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'puntos de vida'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'avg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stddev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o381.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 44 in stage 208.0 failed 1 times, most recent failure: Lost task 44.0 in stage 208.0 (TID 2780, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-63-55ec9c8cbe08>\", line 1, in <lambda>\nZeroDivisionError: float division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor135.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 345, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 334, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-63-55ec9c8cbe08>\", line 1, in <lambda>\nZeroDivisionError: float division by zero\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "result2 = annotated.withColumn('z-score', zscore('puntos de vida','avg', 'stddev'))\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los null values, ver la solucion. Nada del otro mundo.\n",
    "\n",
    "Tenemos dropna que borra todos los que tenga alguna columna vacia. Es configurable, por ejemplo, puedo decir que sólo borre las que tenga todas nulas o le puedo poner un threshold para decir que si me paso de un número de columnas nulas, lo quite, si tiene menos, no. También puedo decir: descártame los que tengan nulos en el campo 'land''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL para consultar contra dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql('SELECT * FROM df3 where land = \"mordor\"') # esto falla, porque no encuentra una tabla df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.registerTempTable('tocoto')\n",
    "spark.sql('SELECT * FROM tocoto where land = \"mordor\"').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interoperacion con Pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas = annotated.toPandas()\n",
    "pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ojo__: esto lo trae a la memoría del nodo driver (del master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el otro sentido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cómo guardar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated.write.csv('out.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto lo guarda en local cuando estoy ejecutando en local. ADemas, lo guarda en un directorio llamadao out.csv con un monton de ficheritos. Cada fichero representa a una partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.0.1  Exercise\n",
    "\n",
    "Repeat the exercise from the previous notebook, but this time with DataFrames.\n",
    "\n",
    "Get stats for all tickets with destination MAD from coupons150720.csv.\n",
    "\n",
    "You will need to extract ticket amounts with destination MAD, and then calculate:\n",
    "\n",
    "Total ticket amounts per origin\n",
    "Top 10 airlines by average amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(tft_number='79062005698500', coupon_number='1', origin='MAA', destination='AUH', carrier='9W', amount=56.79),\n",
       " Row(tft_number='79062005698500', coupon_number='2', origin='AUH', destination='CDG', carrier='9W', amount=84.34),\n",
       " Row(tft_number='79062005924069', coupon_number='1', origin='CJB', destination='MAA', carrier='9W', amount=60.0),\n",
       " Row(tft_number='79065668570385', coupon_number='1', origin='DEL', destination='DXB', carrier='9W', amount=160.63),\n",
       " Row(tft_number='79065668737021', coupon_number='1', origin='AUH', destination='IXE', carrier='9W', amount=152.46)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coupons = spark.sql('''SELECT  \n",
    "            _c0 AS tft_number,\n",
    "            _c1 AS coupon_number,\n",
    "            _c2 AS origin,\n",
    "            _c3 AS destination,\n",
    "            _c4 AS carrier,\n",
    "            CAST(_c6 AS double) AS amount\n",
    "            FROM csv.`data/coupon150720.csv`''')\n",
    "\n",
    "coupons.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total ticket amounts per origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|origin|       sum(amount)|\n",
      "+------+------------------+\n",
      "|   PMI|40547.170000000035|\n",
      "|   YUL|            284.44|\n",
      "|   HEL|           8195.76|\n",
      "|   SXB|            264.46|\n",
      "|   UIO|            8547.6|\n",
      "|   XRY| 9250.230000000001|\n",
      "|   OLB|            1801.5|\n",
      "|   CCS|          94528.68|\n",
      "|   VRN|1020.5399999999998|\n",
      "|   SPC| 7542.700000000002|\n",
      "|   AUH|           4393.42|\n",
      "|   JED|19116.159999999996|\n",
      "|   CMN|           7494.28|\n",
      "|   FRA| 38863.54000000001|\n",
      "|   ALG|14558.570000000002|\n",
      "|   IST|13363.370000000003|\n",
      "|   SDR| 4219.360000000001|\n",
      "|   TXL|15399.790000000003|\n",
      "|   GRU| 87192.64000000001|\n",
      "|   VIE|11666.559999999998|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    coupons \n",
    "    .filter(coupons['destination'] == 'MAD') \n",
    "    .groupby('origin') \n",
    "    .sum('amount') \n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total 10 airlines by average amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(carrier='V0', avg_amount=5418.098666666667),\n",
       " Row(carrier='AC', avg_amount=740.6200000000001),\n",
       " Row(carrier='KE', avg_amount=688.5261538461539),\n",
       " Row(carrier='SV', avg_amount=553.1742553191489),\n",
       " Row(carrier='OB', avg_amount=535.5044444444444),\n",
       " Row(carrier='AR', avg_amount=513.5304761904762),\n",
       " Row(carrier='AV', avg_amount=450.19509554140177),\n",
       " Row(carrier='AM', avg_amount=440.73421052631585),\n",
       " Row(carrier='C2', avg_amount=397.87),\n",
       " Row(carrier='LA', avg_amount=379.9537078651686)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    coupons \n",
    "    .filter(coupons['destination'] == 'MAD') \n",
    "    .groupby('carrier') \n",
    "    .avg('amount') \n",
    "    .withColumnRenamed('avg(amount)','avg_amount') \n",
    "    .orderBy('avg_amount', ascending=False) \n",
    "    .take(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si los datos del csv, por ejemplo, ya filtrados por MAdrid, 0s voy a usar mucho, podríamos cachear los datos. Esto lo cachea en la memoria del cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, podríamos registralo como una tabla para poder usarlo desde `el mundo SQL`. No tendría por qué está cacheado. Si lo está, es como si estuvieramos creando una tabla y consultando de ella y si no, sería como una sub-query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
