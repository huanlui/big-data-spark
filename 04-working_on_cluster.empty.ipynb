{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with computing clusters\n",
    "\n",
    "  <a href = \"http://yogen.io\"><img src=\"http://yogen.io/assets/logo.svg\" alt=\"yogen\" style=\"width: 200px; float: right;\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The idea here is to transmit the idea of how a Data Scientist works remotely.\n",
    "\n",
    "We do it all the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Amazon Web Services\n",
    "\n",
    "![AWS](https://d7umqicpi7263.cloudfront.net/img/product/0f7858eb-0831-4a33-9af9-8e78db6b23d8/c7939ac3-d352-4bee-bf8a-7ee4a2dd2bff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "AWS is widely used\n",
    "\n",
    "You can use an introductory offer to get a taste of it for free.\n",
    "\n",
    "Jeff Bezos intentó que sus productos internos se comportarna como si estuvieran oriuentados a clientes externos. Por ejemplo, que no compartan base de datos, sino que para hacer algo con ese servicio, tengas que pasar por un servicio como pasarela (por otro lado, un buen patrón en ingeniería de SW). De esa forma, le fue fácil compartirlo externamente. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Signing up for aws\n",
    "\n",
    "https://aws.amazon.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### AWS services of interest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- EC2: Elastic Cloud Compute. Allows us to rent virtual machines, computing power, fit to our needs and under several pricing models. Permite lanzar máquinas virtuales en los sistemas de amazon. Fue lo primero que lanzó junto con S3. Te permite tanto máquinas virtuales _limpias_, sólo son el SO (Windows, Linux,...) como algunas con paquetes ya instalados, por ejemplo de ML . Lo bueno de esto, por ejemplo, para una startup, es la elasticidad tremenda que tengo. Tú dices cuánto estás dispuesto a pagar la hora, y el precio para el mismo tipo de rercurso va cambiando según la demanda. Si sube por encima de lo que estás tú dispuesto a pagar, te lo cortan. \n",
    "\n",
    "- S3: Simple Storage Service: Allows us to rent storage capacity to plug into our virtual servers.\n",
    "\n",
    "- EMR: Elastic MapReduce. Simplifies the creation and management of Hadoop/Spark clusters. Me permite crear un clúster con los nodos que yo quiera. Ya con su hadoop o spark configurados. *Equivalente en Google*: `DataProc`\n",
    "\n",
    "- Lambda: Serverless computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating a single instance: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- choosing an operating system\n",
    "\n",
    "- choosing an instance type - spot prices\n",
    "\n",
    "- auto scaling groups\n",
    "\n",
    "- creating a new keypair and storing the private key in .ssh/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Platform\n",
    "\n",
    "![Google Cloud](https://cloud.google.com/_static/5b213a6cb2/images/cloud/cloud-logo.svg)\n",
    "\n",
    "\n",
    "Rival to AWS\n",
    "\n",
    "300$ introductory credit!\n",
    "\n",
    "We will look into it in a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accessing remote computers\n",
    "\n",
    "ssh is your basic tool. You should always use public/private key pair authentication rather than passwords, especially if the ssh port (usually 22) is open to the internet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### public-private keys\n",
    "\n",
    "Generating ssh keys:\n",
    "\n",
    "- [ssh-keygen](https://www.ssh.com/ssh/keygen/)\n",
    "\n",
    "Fijémonos que al crearlo, la clave privada (la que no tiene extensión `.pub`) sólo tiene permisos para el admin, por seguridad.\n",
    "\n",
    "```\n",
    "-rw-------  1 juanluisgarcialopez  staff   1.8K Nov 15 18:36 kschool_jlg\n",
    "(base) ➜  ~ ll kschool_jlg.pub\n",
    "-rw-r--r--  1 juanluisgarcialopez  staff   411B Nov 15 18:36 kschool_jlg.pub\n",
    "(base) ➜  ~\n",
    "```\n",
    "\n",
    "Esto no lo usó:\n",
    "\n",
    "```shell\n",
    "ssh-keygen -y -f $PRIVATE_KEY\n",
    "```\n",
    "\n",
    "Después, en el ordenador remoto, en el archivo /.ssh/authorized_keys hay qye a;adir esta clave publica. (incluir el ssh-rsa del principio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ssh`\n",
    "\n",
    "We need to keep the private key (not the `.pub` file) somewhere where we will not lose it. The standard place is the `~/.ssh/` folder.\n",
    "\n",
    "```shell\n",
    "mkdir -p ~/.ssh\n",
    "mv $key_file ~/.ssh\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`ssh` will let you control a remote machine as if you were typing at its terminal\n",
    "\n",
    "Let's connect to the instance we just created. \n",
    "\n",
    "We need to use the \"identity file\" (private key) to authenticate ourselves:\n",
    "\n",
    "```shell\n",
    "ssh -i $PRIVATE_KEY $REMOTE_USER@$REMOTE_MACHINE\n",
    "```\n",
    "\n",
    "Una de las cosas malas es que si pierdo la conexión, se pierde el proceso largo que estuviera ejecutando. Se usa `screen tmux`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### scp\n",
    "\n",
    "Sending our data to a remote computer. Para enviar ficheros a la máquina virtual o recibirlos. s'olo tengo que cambiar el origen por el destino\n",
    "\n",
    "Let's send coupon150720.csv to the recently created instance.\n",
    "\n",
    "```shell\n",
    "scp -i $PRIVATE_KEY $LOCAL_PATH $REMOTE_USER@$REMOTE_MACHINE:$REMOTE_PATH\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSH config file\n",
    "\n",
    "\n",
    "An SSH config file saves us from typing those long connections every time. It needs to be in `~/.ssh/config` and looks like this:\n",
    "\n",
    "```\n",
    "Host mycluster\n",
    "    User remoteuser\n",
    "    HostName masternodename\n",
    "    IdentityFile ~/.ssh/my-private-key\n",
    "```\n",
    "\n",
    "Once it's there, we can just type\n",
    "\n",
    "```\n",
    "ssh mycluster\n",
    "```\n",
    "and we'll be connected.\n",
    "\n",
    "There are lots and lots of options to ssh we can configure like this. More details [here](https://nerderati.com/2011/03/17/simplify-your-life-with-an-ssh-config-file/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Cloud Platform\n",
    "\n",
    "![Google Cloud](https://cloud.google.com/_static/5b213a6cb2/images/cloud/cloud-logo.svg)\n",
    "\n",
    "\n",
    "Three ways to interact with GCP:\n",
    "\n",
    "* The Google Cloud Platform console (the GUI)\n",
    "\n",
    "* `gcloud` command-line tool\n",
    "\n",
    "* Cloud Dataproc REST API: puedo crear y destruir clusters usando una api rest!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google dataproc\n",
    "\n",
    "[Cloud Dataproc FAQ](https://cloud.google.com/dataproc/docs/resources/faq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the `gcloud` SDK\n",
    "\n",
    "[On Ubuntu/Debian](https://cloud.google.com/sdk/docs/quickstart-debian-ubuntu):\n",
    "\n",
    "```bash\n",
    "# Create environment variable for correct distribution\n",
    "export CLOUD_SDK_REPO=\"cloud-sdk-$(lsb_release -c -s)\"\n",
    "\n",
    "# Add the Cloud SDK distribution URI as a package source\n",
    "echo \"deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n",
    "\n",
    "# Import the Google Cloud Platform public key\n",
    "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "\n",
    "# Update the package list and install the Cloud SDK\n",
    "sudo apt-get update && sudo apt-get install google-cloud-sdk\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the `gcloud` SDK\n",
    "\n",
    "```bash\n",
    "gcloud init\n",
    "```\n",
    "\n",
    "Ejectuando esto puedo ya hacer por consola cualquier cosa de las cosas que se hacen por la consola web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding users to a project\n",
    "\n",
    "[Identity and access management](https://cloud.google.com/iam/docs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a cluster\n",
    "\n",
    "With GUI: Google Cloud Console -> dataproc -> Clusters -> create cluster\n",
    "\n",
    "With SDK: \n",
    "\n",
    "```bash\n",
    "gcloud dataproc clusters create $CLUSTERNAME --region $REGION\n",
    "```\n",
    "\n",
    "Many more options available. You can explore them within the SDK or through the GUI.\n",
    "\n",
    "[Creating a Cluster](https://cloud.google.com/dataproc/docs/guides/create-cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading data a GCP cluster\n",
    "\n",
    "2 Options:\n",
    "\n",
    "* scp to the master node\n",
    "\n",
    "* Upload the data to Google Cloud Storage, then use `gs://` as a path prefix on your script\n",
    "\n",
    "    * First, you'll need to [create a storage bucket].\n",
    "    \n",
    "[create a storage bucket]: https://cloud.google.com/storage/docs/creating-buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a storage bucket\n",
    "\n",
    "```bash\n",
    "gsutil mb -p $PROJECT_NAME  gs://bucket-name\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading your data\n",
    "\n",
    "```bash\n",
    "gsutil cp [LOCAL_OBJECT_LOCATION] gs://[DESTINATION_BUCKET_NAME]/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting a job to Google dataproc\n",
    "\n",
    "To submit a PySpark job, run:\n",
    "\n",
    "```bash\n",
    "  $ gcloud dataproc jobs submit pyspark --cluster my_cluster \\\n",
    "      my_script.py -- arg1 arg2\n",
    "      \n",
    "```\n",
    "\n",
    "https://cloud.google.com/sdk/gcloud/reference/dataproc/jobs/submit/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Storage: HDFS (Hadoop Distributed File System)\n",
    "\n",
    "El HDFS está ligado al clúster. Si este se elimina, el HFDS fuere con él. En constraste, el Google Storage no. \n",
    "\n",
    "The \"internal storage\" of a Spark cluster is usually HDFS. In GCP we don't worry too much about it because we use Google Storage, but it is important to be able basic concepts about it in case we use another cloud provider or have an internal cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Assumptions in [HDFS design]:\n",
    "\n",
    "* The system is built from many inexpensive commodity components that often fail. El sistema tiene un montón de componentes nada especiales y que por tanto fallan a menudo. \n",
    "\n",
    "* The system stores a modest number of large files. \n",
    "\n",
    "* The workloads primarily consist of two kinds of reads: large streaming reads and small random reads. \n",
    "\n",
    "* The workloads also have many large, sequential writes that append data to files.\n",
    "\n",
    "* The system must efficiently implement well-defined semantics for multiple clients that concurrently append to the same file.\n",
    "\n",
    "* High sustained bandwidth is more important than low latency. \n",
    "\n",
    "[HDFS design]: https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "![HDFS](https://hadoop.apache.org/docs/r2.7.3/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Blocks` = en lo que está partido el archivo, que puede estar partido\n",
    "`Rack` = es consciente que os nodos están en racks, y sabe que si dos nodos están en distintos racks, la comunicación irá más lenta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### hdfs dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mimics the shell, but with a few differences:\n",
    "\n",
    "* We call shell commands as options to a module named hdfs dfs\n",
    "\n",
    "* There is no concept of a current working directory (therefore, no cd command)\n",
    "\n",
    "* It has some annoying inconsistencies with regular bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```shell\n",
    "\n",
    "[hadoop@masternode ~]$ hdfs dfs \n",
    "\n",
    "Usage: hadoop fs [generic options]\n",
    "\t[-appendToFile <localsrc> ... <dst>]\n",
    "\t[-cat [-ignoreCrc] <src> ...]\n",
    "\t[-checksum <src> ...]\n",
    "\t[-chgrp [-R] GROUP PATH...]\n",
    "\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n",
    "\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n",
    "\t[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]\n",
    "\t[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
    "\t[-count [-q] [-h] <path> ...]\n",
    "\t[-cp [-f] [-p | -p[topax]] <src> ... <dst>]\n",
    "\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n",
    "\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n",
    "\t[-df [-h] [<path> ...]]\n",
    "\t[-du [-s] [-h] <path> ...]\n",
    "\t[-expunge]\n",
    "\t[-find <path> ... <expression> ...]\n",
    "\t[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n",
    "\t[-getfacl [-R] <path>]\n",
    "\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n",
    "\t[-getmerge [-nl] <src> <localdst>]\n",
    "\t[-help [cmd ...]]\n",
    "\t[-ls [-d] [-h] [-R] [<path> ...]]\n",
    "\t[-mkdir [-p] <path> ...]\n",
    "\t[-moveFromLocal <localsrc> ... <dst>]\n",
    "\t[-moveToLocal <src> <localdst>]\n",
    "\t[-mv <src> ... <dst>]\n",
    "\t[-put [-f] [-p] [-l] <localsrc> ... <dst>]\n",
    "\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n",
    "\t[-rm [-f] [-r|-R] [-skipTrash] <src> ...]\n",
    "\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n",
    "\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n",
    "\t[-setfattr {-n name [-v value] | -x name} <path>]\n",
    "\t[-setrep [-R] [-w] <rep> <path> ...]\n",
    "\t[-stat [format] <path> ...]\n",
    "\t[-tail [-f] <file>]\n",
    "\t[-test -[defsz] <path>]\n",
    "\t[-text [-ignoreCrc] <src> ...]\n",
    "\t[-touchz <path> ...]\n",
    "\t[-truncate [-w] <length> <path> ...]\n",
    "\t[-usage [cmd ...]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Try:\n",
    "\n",
    "```shell\n",
    "user@gateway$ hdfs dfs -ls\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why does it return nothing?\n",
    "\n",
    "Now try:\n",
    "\n",
    "```shell\n",
    "user@gateway$ hdfs dfs -ls /\n",
    "user@gateway$ hdfs dfs -ls /user\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `hdfs dfs -mkdir`\n",
    "\n",
    "Create a folder inside your hdfs home folder that is called \"data\", on your own\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `hdfs dfs -put`\n",
    "\n",
    "By analogy with ls, can you guess where the\n",
    "`$LOCAL_FILE` will be put if I do this? (don't do it)\n",
    "                                       \n",
    "```shell\n",
    "user@gateway$ hdfs dfs -put $LOCAL_FILE\n",
    "\n",
    "```\n",
    "                                       \n",
    "                                       \n",
    "Now, put the file in hdfs, inside your \"data\" folder:\n",
    "```shell\n",
    "user@gateway$ hdfs dfs -put $LOCAL_FILE $HDFS_FOLDER\n",
    "```\n",
    " \n",
    "                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `hdfs dfs -get` / `hdfs dfs -cat`\n",
    "\n",
    "If you do any kind of work in HDFS, eventually you'll need to get something out of it!\n",
    "\n",
    "```shell\n",
    "user@gateway$ hdfs dfs -cat $HDFS_FILE\n",
    "```\n",
    "\n",
    "However, you might only need take a peek into the contents of a file:\n",
    "\n",
    "```shell\n",
    "user@gateway$ hdfs dfs -get $HDFS_FILE\n",
    "```\n",
    "\n",
    "The neat thing about hdfs dfs -cat is that it outputs to stdout, so you can chain it to all your favorite shell pipelines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Other useful hadoop filesystem commands:\n",
    "    \n",
    "```shell\n",
    "user@gateway$ hdfs dfs -getmerge $HDFS_GLOB $LOCAL_FILE\n",
    "user@gateway$ hdfs dfs -stat $HDFS_FILE\n",
    "user@gateway$ hdfs dfs -tail $HDFS_FILE\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Much more at:\n",
    "https://hadoop.apache.org/docs/r2.8.3/hadoop-project-dist/hadoop-common/FileSystemShell.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## spark-submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### ```mysparkjob.py```\n",
    "\n",
    "\n",
    "```python\n",
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "import sys\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: mysparkjob arg1 arg2 \", file=sys.stderr)\n",
    "        exit(-1)\n",
    "    sc = SparkContext(appName=\"MyTestJob\")\n",
    "    dataTextAll = sc.textFile(sys.argv[1])\n",
    "    dataRDD = dataTextAll.filter(lambda line: line.startswith('79065'))\n",
    "    dataRDD.saveAsTextFile(sys.argv[2])\n",
    "    sc.stop()\n",
    "```\n",
    "\n",
    "Just a simple Spark job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Runnning our Spark app\n",
    "\n",
    "```shell\n",
    "./bin/spark-submit \\\n",
    "    mysparkjob.py \\\n",
    "    data/coupon150720.csv \\\n",
    "    test.csv\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once it runs, what is test.csv? How would you get it back on the local file system?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise \n",
    "\n",
    "Adapt our exercise from notebook 02 to run in the cluster. Remember:\n",
    "\n",
    "Get stats for all tickets with destination MAD from `coupons150720.csv`. You will need to extract ticket amounts with destination MAD, and then calculate:\n",
    "\n",
    "* Total ticket amounts per origin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/coupon150720.csv'\n",
    "df = spark.sql(f'''SELECT  \n",
    "            _c0 AS tft_number,\n",
    "            _c1 AS coupon_number,\n",
    "            _c2 AS origin,\n",
    "            _c3 AS destination,\n",
    "            _c4 AS carrier,\n",
    "            CAST(_c6 AS double) AS amount\n",
    "            FROM csv.`{path}`''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[destination: string, sum_amount: double]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amounts = (\n",
    "    df\n",
    "    .where(df['origin'] == 'MAD')\n",
    "    .groupBy('destination')\n",
    "    .sum('amount')\n",
    "    .withColumnRenamed('sum(amount)','sum_amount') \n",
    ")\n",
    "\n",
    "amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(destination='PMI', sum_amount=37475.79000000004),\n",
       " Row(destination='HEL', sum_amount=8268.34),\n",
       " Row(destination='SXB', sum_amount=264.46),\n",
       " Row(destination='UIO', sum_amount=14339.710000000005),\n",
       " Row(destination='XRY', sum_amount=8611.57),\n",
       " Row(destination='OLB', sum_amount=785.98),\n",
       " Row(destination='CCS', sum_amount=53432.03),\n",
       " Row(destination='VRN', sum_amount=2469.480000000001),\n",
       " Row(destination='SPC', sum_amount=6033.049999999999),\n",
       " Row(destination='AUH', sum_amount=8775.109999999999)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amounts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo copio a un py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\r\n",
      "from pyspark.sql import SparkSession,types, functions\r\n",
      "import sys\r\n",
      "\r\n",
      "    \r\n",
      "if __name__=='__main__':\r\n",
      "    \r\n",
      "    in_ = sys.argv[1]\r\n",
      "    out_ = sys.argv[2]\r\n",
      "    \r\n",
      "    spark = SparkSession.builder.getOrCreate()\r\n",
      "\r\n",
      "    df = spark.sql(f'''SELECT  \r\n",
      "            _c0 AS tft_number,\r\n",
      "            _c1 AS coupon_number,\r\n",
      "            _c2 AS origin,\r\n",
      "            _c3 AS destination,\r\n",
      "            _c4 AS carrier,\r\n",
      "            CAST(_c6 AS double) AS amount\r\n",
      "            FROM csv.`{in_}`''')\r\n",
      "\r\n",
      "    amounts = (\r\n",
      "    df\r\n",
      "    .where(df['origin'] == 'MAD')\r\n",
      "    .groupBy('destination')\r\n",
      "    .sum('amount')\r\n",
      "    .withColumnRenamed('sum(amount)','sum_amount') \r\n",
      "    )\r\n",
      "\r\n",
      "    amounts.write.json(out_)\r\n",
      "    spark.stop()\r\n"
     ]
    }
   ],
   "source": [
    "cat sum_amount.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Este no funciona porque `f'{valor}'` es de python 3, y el cluster tiene python 2`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que estamos usando `sum_amount_2.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probarlo desde el notebook, antes de subirlo (aunque esto es python 3 y el otro 2, no te asegura 100% que vaya bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "19/11/16 09:43:22 INFO SparkContext: Running Spark version 2.4.4\n",
      "19/11/16 09:43:22 INFO SparkContext: Submitted application: sum_amount.py\n",
      "19/11/16 09:43:22 INFO SecurityManager: Changing view acls to: juanluisgarcialopez\n",
      "19/11/16 09:43:22 INFO SecurityManager: Changing modify acls to: juanluisgarcialopez\n",
      "19/11/16 09:43:22 INFO SecurityManager: Changing view acls groups to: \n",
      "19/11/16 09:43:22 INFO SecurityManager: Changing modify acls groups to: \n",
      "19/11/16 09:43:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(juanluisgarcialopez); groups with view permissions: Set(); users  with modify permissions: Set(juanluisgarcialopez); groups with modify permissions: Set()\n",
      "19/11/16 09:43:23 INFO Utils: Successfully started service 'sparkDriver' on port 49974.\n",
      "19/11/16 09:43:23 INFO SparkEnv: Registering MapOutputTracker\n",
      "19/11/16 09:43:23 INFO SparkEnv: Registering BlockManagerMaster\n",
      "19/11/16 09:43:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "19/11/16 09:43:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "19/11/16 09:43:23 INFO DiskBlockManager: Created local directory at /private/var/folders/k4/33yt9vyn3_x0v6tsv21x51_w0000gn/T/blockmgr-649442cc-3a22-464b-88ed-27fc752a7d25\n",
      "19/11/16 09:43:23 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "19/11/16 09:43:23 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "19/11/16 09:43:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "19/11/16 09:43:23 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "19/11/16 09:43:23 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "19/11/16 09:43:23 INFO Utils: Successfully started service 'SparkUI' on port 4043.\n",
      "19/11/16 09:43:23 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://juans-mbp:4043\n",
      "19/11/16 09:43:23 INFO Executor: Starting executor ID driver on host localhost\n",
      "19/11/16 09:43:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49975.\n",
      "19/11/16 09:43:23 INFO NettyBlockTransferService: Server created on juans-mbp:49975\n",
      "19/11/16 09:43:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "19/11/16 09:43:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, juans-mbp, 49975, None)\n",
      "19/11/16 09:43:23 INFO BlockManagerMasterEndpoint: Registering block manager juans-mbp:49975 with 366.3 MB RAM, BlockManagerId(driver, juans-mbp, 49975, None)\n",
      "19/11/16 09:43:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, juans-mbp, 49975, None)\n",
      "19/11/16 09:43:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, juans-mbp, 49975, None)\n",
      "19/11/16 09:43:23 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/spark-warehouse').\n",
      "19/11/16 09:43:23 INFO SharedState: Warehouse path is 'file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/spark-warehouse'.\n",
      "19/11/16 09:43:24 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "19/11/16 09:43:26 INFO FileSourceStrategy: Pruning directories with: \n",
      "19/11/16 09:43:26 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#6, None)) > 0)\n",
      "19/11/16 09:43:26 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "19/11/16 09:43:26 INFO FileSourceScanExec: Pushed Filters: \n",
      "19/11/16 09:43:26 INFO CodeGenerator: Code generated in 178.036006 ms\n",
      "19/11/16 09:43:26 INFO CodeGenerator: Code generated in 15.693918 ms\n",
      "19/11/16 09:43:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 277.3 KB, free 366.0 MB)\n",
      "19/11/16 09:43:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.3 KB, free 366.0 MB)\n",
      "19/11/16 09:43:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on juans-mbp:49975 (size: 23.3 KB, free: 366.3 MB)\n",
      "19/11/16 09:43:27 INFO SparkContext: Created broadcast 0 from sql at NativeMethodAccessorImpl.java:0\n",
      "19/11/16 09:43:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7209300 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "19/11/16 09:43:27 INFO SparkContext: Starting job: sql at NativeMethodAccessorImpl.java:0\n",
      "19/11/16 09:43:27 INFO DAGScheduler: Got job 0 (sql at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "19/11/16 09:43:27 INFO DAGScheduler: Final stage: ResultStage 0 (sql at NativeMethodAccessorImpl.java:0)\n",
      "19/11/16 09:43:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "19/11/16 09:43:27 INFO DAGScheduler: Missing parents: List()\n",
      "19/11/16 09:43:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at sql at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "19/11/16 09:43:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 8.8 KB, free 366.0 MB)\n",
      "19/11/16 09:43:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KB, free 366.0 MB)\n",
      "19/11/16 09:43:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on juans-mbp:49975 (size: 4.5 KB, free: 366.3 MB)\n",
      "19/11/16 09:43:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161\n",
      "19/11/16 09:43:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at sql at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "19/11/16 09:43:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks\n",
      "19/11/16 09:43:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 8304 bytes)\n",
      "19/11/16 09:43:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "19/11/16 09:43:27 INFO FileScanRDD: Reading File path: file:///Users/juanluisgarcialopez/repos-datascience/big-data-spark/data/coupon150720.csv, range: 0-7209300, partition values: [empty row]\n",
      "19/11/16 09:43:27 INFO CodeGenerator: Code generated in 8.620016 ms\n",
      "19/11/16 09:43:27 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1331 bytes result sent to driver\n",
      "19/11/16 09:43:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 127 ms on localhost (executor driver) (1/1)\n",
      "19/11/16 09:43:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "19/11/16 09:43:27 INFO DAGScheduler: ResultStage 0 (sql at NativeMethodAccessorImpl.java:0) finished in 0.212 s\n",
      "19/11/16 09:43:27 INFO DAGScheduler: Job 0 finished: sql at NativeMethodAccessorImpl.java:0, took 0.253294 s\n",
      "19/11/16 09:43:27 INFO FileSourceStrategy: Pruning directories with: \n",
      "19/11/16 09:43:27 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "19/11/16 09:43:27 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "19/11/16 09:43:27 INFO FileSourceScanExec: Pushed Filters: \n",
      "19/11/16 09:43:27 INFO CodeGenerator: Code generated in 8.15769 ms\n",
      "19/11/16 09:43:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 277.3 KB, free 365.7 MB)\n",
      "19/11/16 09:43:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.3 KB, free 365.7 MB)\n",
      "19/11/16 09:43:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on juans-mbp:49975 (size: 23.3 KB, free: 366.3 MB)\n",
      "19/11/16 09:43:27 INFO SparkContext: Created broadcast 2 from sql at NativeMethodAccessorImpl.java:0\n",
      "19/11/16 09:43:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7209300 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 18\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 21\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 24\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 30\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 11\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 14\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 26\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 31\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 7\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 10\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 12\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 9\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 19\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 20\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 27\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 8\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 16\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 15\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 22\n",
      "19/11/16 09:43:27 INFO BlockManagerInfo: Removed broadcast_1_piece0 on juans-mbp:49975 in memory (size: 4.5 KB, free: 366.3 MB)\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 23\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 29\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 17\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 25\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 28\n",
      "19/11/16 09:43:27 INFO ContextCleaner: Cleaned accumulator 13\n",
      "19/11/16 09:43:27 INFO FileSourceStrategy: Pruning directories with: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:27 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(_c2#18),(_c2#18 = MAD)\n",
      "19/11/16 09:43:27 INFO FileSourceStrategy: Output Data Schema: struct<_c2: string, _c3: string, _c6: string ... 1 more fields>\n",
      "19/11/16 09:43:27 INFO FileSourceScanExec: Pushed Filters: IsNotNull(_c2),EqualTo(_c2,MAD)\n",
      "19/11/16 09:43:27 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:27 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:27 INFO CodeGenerator: Code generated in 29.562143 ms\n",
      "19/11/16 09:43:27 INFO CodeGenerator: Code generated in 43.282683 ms\n",
      "19/11/16 09:43:27 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 277.3 KB, free 365.4 MB)\n",
      "19/11/16 09:43:27 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.3 KB, free 365.4 MB)\n",
      "19/11/16 09:43:27 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on juans-mbp:49975 (size: 23.3 KB, free: 366.2 MB)\n",
      "19/11/16 09:43:27 INFO SparkContext: Created broadcast 3 from json at NativeMethodAccessorImpl.java:0\n",
      "19/11/16 09:43:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 7209300 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "19/11/16 09:43:27 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0\n",
      "19/11/16 09:43:27 INFO DAGScheduler: Registering RDD 11 (json at NativeMethodAccessorImpl.java:0)\n",
      "19/11/16 09:43:27 INFO DAGScheduler: Got job 1 (json at NativeMethodAccessorImpl.java:0) with 200 output partitions\n",
      "19/11/16 09:43:27 INFO DAGScheduler: Final stage: ResultStage 2 (json at NativeMethodAccessorImpl.java:0)\n",
      "19/11/16 09:43:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
      "19/11/16 09:43:27 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 1)\n",
      "19/11/16 09:43:27 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "19/11/16 09:43:27 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 32.8 KB, free 365.4 MB)\n",
      "19/11/16 09:43:27 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 15.6 KB, free 365.4 MB)\n",
      "19/11/16 09:43:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on juans-mbp:49975 (size: 15.6 KB, free: 366.2 MB)\n",
      "19/11/16 09:43:27 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161\n",
      "19/11/16 09:43:27 INFO DAGScheduler: Submitting 12 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[11] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "19/11/16 09:43:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 12 tasks\n",
      "19/11/16 09:43:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 8293 bytes)\n",
      "19/11/16 09:43:27 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 8293 bytes)\n",
      "19/11/16 09:43:27 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, localhost, executor driver, partition 2, PROCESS_LOCAL, 8293 bytes)\n",
      "19/11/16 09:43:27 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, localhost, executor driver, partition 3, PROCESS_LOCAL, 8293 bytes)\n",
      "19/11/16 09:43:27 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5, localhost, executor driver, partition 4, PROCESS_LOCAL, 8293 bytes)\n",
      "19/11/16 09:43:27 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6, localhost, executor driver, partition 5, PROCESS_LOCAL, 8293 bytes)\n",
      "19/11/16 09:43:27 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7, localhost, executor driver, partition 6, PROCESS_LOCAL, 8293 bytes)\n",
      "19/11/16 09:43:27 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8, localhost, executor driver, partition 7, PROCESS_LOCAL, 8293 bytes)\n",
      "19/11/16 09:43:27 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9, localhost, executor driver, partition 8, PROCESS_LOCAL, 8293 bytes)\n",
      "19/11/16 09:43:27 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10, localhost, executor driver, partition 9, PROCESS_LOCAL, 8293 bytes)\n",
      "19/11/16 09:43:27 INFO TaskSetManager: Starting task 10.0 in stage 1.0 (TID 11, localhost, executor driver, partition 10, PROCESS_LOCAL, 8293 bytes)\n",
      "19/11/16 09:43:27 INFO TaskSetManager: Starting task 11.0 in stage 1.0 (TID 12, localhost, executor driver, partition 11, PROCESS_LOCAL, 8293 bytes)\n",
      "19/11/16 09:43:27 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "19/11/16 09:43:27 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
      "19/11/16 09:43:27 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)\n",
      "19/11/16 09:43:27 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)\n",
      "19/11/16 09:43:27 INFO Executor: Running task 4.0 in stage 1.0 (TID 5)\n",
      "19/11/16 09:43:27 INFO Executor: Running task 5.0 in stage 1.0 (TID 6)\n",
      "19/11/16 09:43:27 INFO Executor: Running task 6.0 in stage 1.0 (TID 7)\n",
      "19/11/16 09:43:27 INFO Executor: Running task 7.0 in stage 1.0 (TID 8)\n",
      "19/11/16 09:43:27 INFO Executor: Running task 8.0 in stage 1.0 (TID 9)\n",
      "19/11/16 09:43:27 INFO Executor: Running task 9.0 in stage 1.0 (TID 10)\n",
      "19/11/16 09:43:27 INFO Executor: Running task 10.0 in stage 1.0 (TID 11)\n",
      "19/11/16 09:43:27 INFO Executor: Running task 11.0 in stage 1.0 (TID 12)\n",
      "19/11/16 09:43:27 INFO CodeGenerator: Code generated in 6.504341 ms\n",
      "19/11/16 09:43:28 INFO CodeGenerator: Code generated in 6.374478 ms\n",
      "19/11/16 09:43:28 INFO CodeGenerator: Code generated in 9.184339 ms\n",
      "19/11/16 09:43:28 INFO FileScanRDD: Reading File path: file:///Users/juanluisgarcialopez/repos-datascience/big-data-spark/data/coupon150720.csv, range: 0-7209300, partition values: [empty row]\n",
      "19/11/16 09:43:28 INFO FileScanRDD: Reading File path: file:///Users/juanluisgarcialopez/repos-datascience/big-data-spark/data/coupon150720.csv, range: 64883700-72093000, partition values: [empty row]\n",
      "19/11/16 09:43:28 INFO FileScanRDD: Reading File path: file:///Users/juanluisgarcialopez/repos-datascience/big-data-spark/data/coupon150720.csv, range: 21627900-28837200, partition values: [empty row]\n",
      "19/11/16 09:43:28 INFO FileScanRDD: Reading File path: file:///Users/juanluisgarcialopez/repos-datascience/big-data-spark/data/coupon150720.csv, range: 43255800-50465100, partition values: [empty row]\n",
      "19/11/16 09:43:28 INFO FileScanRDD: Reading File path: file:///Users/juanluisgarcialopez/repos-datascience/big-data-spark/data/coupon150720.csv, range: 36046500-43255800, partition values: [empty row]\n",
      "19/11/16 09:43:28 INFO FileScanRDD: Reading File path: file:///Users/juanluisgarcialopez/repos-datascience/big-data-spark/data/coupon150720.csv, range: 57674400-64883700, partition values: [empty row]\n",
      "19/11/16 09:43:28 INFO FileScanRDD: Reading File path: file:///Users/juanluisgarcialopez/repos-datascience/big-data-spark/data/coupon150720.csv, range: 72093000-79302300, partition values: [empty row]\n",
      "19/11/16 09:43:28 INFO FileScanRDD: Reading File path: file:///Users/juanluisgarcialopez/repos-datascience/big-data-spark/data/coupon150720.csv, range: 79302300-82317299, partition values: [empty row]\n",
      "19/11/16 09:43:28 INFO FileScanRDD: Reading File path: file:///Users/juanluisgarcialopez/repos-datascience/big-data-spark/data/coupon150720.csv, range: 50465100-57674400, partition values: [empty row]\n",
      "19/11/16 09:43:28 INFO FileScanRDD: Reading File path: file:///Users/juanluisgarcialopez/repos-datascience/big-data-spark/data/coupon150720.csv, range: 14418600-21627900, partition values: [empty row]\n",
      "19/11/16 09:43:28 INFO FileScanRDD: Reading File path: file:///Users/juanluisgarcialopez/repos-datascience/big-data-spark/data/coupon150720.csv, range: 28837200-36046500, partition values: [empty row]\n",
      "19/11/16 09:43:28 INFO FileScanRDD: Reading File path: file:///Users/juanluisgarcialopez/repos-datascience/big-data-spark/data/coupon150720.csv, range: 7209300-14418600, partition values: [empty row]\n",
      "19/11/16 09:43:28 INFO CodeGenerator: Code generated in 8.184495 ms\n",
      "19/11/16 09:43:28 INFO ContextCleaner: Cleaned accumulator 37\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 11.0 in stage 1.0 (TID 12). 2502 bytes result sent to driver\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID 12) in 550 ms on localhost (executor driver) (1/12)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:28 INFO Executor: Finished task 10.0 in stage 1.0 (TID 11). 2459 bytes result sent to driver\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID 11) in 697 ms on localhost (executor driver) (2/12)\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 9.0 in stage 1.0 (TID 10). 2459 bytes result sent to driver\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 710 ms on localhost (executor driver) (3/12)\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 4.0 in stage 1.0 (TID 5). 2459 bytes result sent to driver\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 725 ms on localhost (executor driver) (4/12)\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 7.0 in stage 1.0 (TID 8). 2502 bytes result sent to driver\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 745 ms on localhost (executor driver) (5/12)\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 6.0 in stage 1.0 (TID 7). 2459 bytes result sent to driver\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 752 ms on localhost (executor driver) (6/12)\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 5.0 in stage 1.0 (TID 6). 2459 bytes result sent to driver\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 755 ms on localhost (executor driver) (7/12)\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 8.0 in stage 1.0 (TID 9). 2459 bytes result sent to driver\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 761 ms on localhost (executor driver) (8/12)\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2459 bytes result sent to driver\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 775 ms on localhost (executor driver) (9/12)\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 2459 bytes result sent to driver\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 806 ms on localhost (executor driver) (10/12)\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 2459 bytes result sent to driver\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 821 ms on localhost (executor driver) (11/12)\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 2459 bytes result sent to driver\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 862 ms on localhost (executor driver) (12/12)\n",
      "19/11/16 09:43:28 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "19/11/16 09:43:28 INFO DAGScheduler: ShuffleMapStage 1 (json at NativeMethodAccessorImpl.java:0) finished in 0.872 s\n",
      "19/11/16 09:43:28 INFO DAGScheduler: looking for newly runnable stages\n",
      "19/11/16 09:43:28 INFO DAGScheduler: running: Set()\n",
      "19/11/16 09:43:28 INFO DAGScheduler: waiting: Set(ResultStage 2)\n",
      "19/11/16 09:43:28 INFO DAGScheduler: failed: Set()\n",
      "19/11/16 09:43:28 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at json at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "19/11/16 09:43:28 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 167.1 KB, free 365.2 MB)\n",
      "19/11/16 09:43:28 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 62.1 KB, free 365.1 MB)\n",
      "19/11/16 09:43:28 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on juans-mbp:49975 (size: 62.1 KB, free: 366.2 MB)\n",
      "19/11/16 09:43:28 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161\n",
      "19/11/16 09:43:28 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "19/11/16 09:43:28 INFO TaskSchedulerImpl: Adding task set 2.0 with 200 tasks\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 13, localhost, executor driver, partition 5, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 14, localhost, executor driver, partition 10, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 13.0 in stage 2.0 (TID 15, localhost, executor driver, partition 13, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 16.0 in stage 2.0 (TID 16, localhost, executor driver, partition 16, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 17.0 in stage 2.0 (TID 17, localhost, executor driver, partition 17, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 19.0 in stage 2.0 (TID 18, localhost, executor driver, partition 19, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 20.0 in stage 2.0 (TID 19, localhost, executor driver, partition 20, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 22.0 in stage 2.0 (TID 20, localhost, executor driver, partition 22, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 23.0 in stage 2.0 (TID 21, localhost, executor driver, partition 23, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 24.0 in stage 2.0 (TID 22, localhost, executor driver, partition 24, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 25.0 in stage 2.0 (TID 23, localhost, executor driver, partition 25, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 26.0 in stage 2.0 (TID 24, localhost, executor driver, partition 26, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:28 INFO Executor: Running task 10.0 in stage 2.0 (TID 14)\n",
      "19/11/16 09:43:28 INFO Executor: Running task 13.0 in stage 2.0 (TID 15)\n",
      "19/11/16 09:43:28 INFO Executor: Running task 25.0 in stage 2.0 (TID 23)\n",
      "19/11/16 09:43:28 INFO Executor: Running task 26.0 in stage 2.0 (TID 24)\n",
      "19/11/16 09:43:28 INFO Executor: Running task 24.0 in stage 2.0 (TID 22)\n",
      "19/11/16 09:43:28 INFO Executor: Running task 19.0 in stage 2.0 (TID 18)\n",
      "19/11/16 09:43:28 INFO Executor: Running task 23.0 in stage 2.0 (TID 21)\n",
      "19/11/16 09:43:28 INFO Executor: Running task 22.0 in stage 2.0 (TID 20)\n",
      "19/11/16 09:43:28 INFO Executor: Running task 16.0 in stage 2.0 (TID 16)\n",
      "19/11/16 09:43:28 INFO Executor: Running task 17.0 in stage 2.0 (TID 17)\n",
      "19/11/16 09:43:28 INFO Executor: Running task 20.0 in stage 2.0 (TID 19)\n",
      "19/11/16 09:43:28 INFO Executor: Running task 5.0 in stage 2.0 (TID 13)\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000026_24\r\n",
      "19/11/16 09:43:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000016_16\r\n",
      "19/11/16 09:43:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000020_19\r\n",
      "19/11/16 09:43:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000017_17\r\n",
      "19/11/16 09:43:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000024_22\r\n",
      "19/11/16 09:43:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000025_23\r\n",
      "19/11/16 09:43:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000013_15\r\n",
      "19/11/16 09:43:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000010_14\r\n",
      "19/11/16 09:43:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000005_13\r\n",
      "19/11/16 09:43:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000023_21\r\n",
      "19/11/16 09:43:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000022_20\r\n",
      "19/11/16 09:43:28 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000019_18\r\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 19.0 in stage 2.0 (TID 18). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 27.0 in stage 2.0 (TID 25, localhost, executor driver, partition 27, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 10.0 in stage 2.0 (TID 14). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 17.0 in stage 2.0 (TID 17). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 16.0 in stage 2.0 (TID 16). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 25.0 in stage 2.0 (TID 23). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 24.0 in stage 2.0 (TID 22). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:28 INFO Executor: Running task 27.0 in stage 2.0 (TID 25)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 28.0 in stage 2.0 (TID 26, localhost, executor driver, partition 28, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 20.0 in stage 2.0 (TID 19). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 26.0 in stage 2.0 (TID 24). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 22.0 in stage 2.0 (TID 20). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 23.0 in stage 2.0 (TID 21). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:28 INFO Executor: Running task 28.0 in stage 2.0 (TID 26)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 19.0 in stage 2.0 (TID 18) in 88 ms on localhost (executor driver) (1/200)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 14) in 90 ms on localhost (executor driver) (2/200)\r\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 5.0 in stage 2.0 (TID 13). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:28 INFO Executor: Finished task 13.0 in stage 2.0 (TID 15). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 29.0 in stage 2.0 (TID 27, localhost, executor driver, partition 29, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:28 INFO Executor: Running task 29.0 in stage 2.0 (TID 27)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 30.0 in stage 2.0 (TID 28, localhost, executor driver, partition 30, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 31.0 in stage 2.0 (TID 29, localhost, executor driver, partition 31, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 17.0 in stage 2.0 (TID 17) in 92 ms on localhost (executor driver) (3/200)\r\n",
      "19/11/16 09:43:28 INFO Executor: Running task 30.0 in stage 2.0 (TID 28)\r\n",
      "19/11/16 09:43:28 INFO Executor: Running task 31.0 in stage 2.0 (TID 29)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 16.0 in stage 2.0 (TID 16) in 92 ms on localhost (executor driver) (4/200)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 25.0 in stage 2.0 (TID 23) in 91 ms on localhost (executor driver) (5/200)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 33.0 in stage 2.0 (TID 30, localhost, executor driver, partition 33, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:28 INFO Executor: Running task 33.0 in stage 2.0 (TID 30)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 39.0 in stage 2.0 (TID 31, localhost, executor driver, partition 39, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 24.0 in stage 2.0 (TID 22) in 92 ms on localhost (executor driver) (6/200)\r\n",
      "19/11/16 09:43:28 INFO Executor: Running task 39.0 in stage 2.0 (TID 31)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 40.0 in stage 2.0 (TID 32, localhost, executor driver, partition 40, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 20.0 in stage 2.0 (TID 19) in 93 ms on localhost (executor driver) (7/200)\r\n",
      "19/11/16 09:43:28 INFO Executor: Running task 40.0 in stage 2.0 (TID 32)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 43.0 in stage 2.0 (TID 33, localhost, executor driver, partition 43, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 26.0 in stage 2.0 (TID 24) in 93 ms on localhost (executor driver) (8/200)\r\n",
      "19/11/16 09:43:28 INFO Executor: Running task 43.0 in stage 2.0 (TID 33)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 46.0 in stage 2.0 (TID 34, localhost, executor driver, partition 46, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 22.0 in stage 2.0 (TID 20) in 94 ms on localhost (executor driver) (9/200)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 48.0 in stage 2.0 (TID 35, localhost, executor driver, partition 48, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 23.0 in stage 2.0 (TID 21) in 94 ms on localhost (executor driver) (10/200)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Starting task 49.0 in stage 2.0 (TID 36, localhost, executor driver, partition 49, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 13) in 97 ms on localhost (executor driver) (11/200)\r\n",
      "19/11/16 09:43:28 INFO TaskSetManager: Finished task 13.0 in stage 2.0 (TID 15) in 96 ms on localhost (executor driver) (12/200)\r\n",
      "19/11/16 09:43:28 INFO Executor: Running task 46.0 in stage 2.0 (TID 34)\r\n",
      "19/11/16 09:43:28 INFO Executor: Running task 48.0 in stage 2.0 (TID 35)\r\n",
      "19/11/16 09:43:28 INFO Executor: Running task 49.0 in stage 2.0 (TID 36)\r\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\r\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:28 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\r\n",
      "19/11/16 09:43:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:28 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:28 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000039_31\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 39.0 in stage 2.0 (TID 31). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000033_30\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 52.0 in stage 2.0 (TID 37, localhost, executor driver, partition 52, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 33.0 in stage 2.0 (TID 30). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 52.0 in stage 2.0 (TID 37)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 54.0 in stage 2.0 (TID 38, localhost, executor driver, partition 54, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000040_32\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 39.0 in stage 2.0 (TID 31) in 44 ms on localhost (executor driver) (13/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 54.0 in stage 2.0 (TID 38)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 33.0 in stage 2.0 (TID 30) in 45 ms on localhost (executor driver) (14/200)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000043_33\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000028_26\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 43.0 in stage 2.0 (TID 33). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 40.0 in stage 2.0 (TID 32). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 56.0 in stage 2.0 (TID 39, localhost, executor driver, partition 56, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 57.0 in stage 2.0 (TID 40, localhost, executor driver, partition 57, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 56.0 in stage 2.0 (TID 39)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 57.0 in stage 2.0 (TID 40)\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000027_25\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 28.0 in stage 2.0 (TID 26). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000049_36\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000048_35\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 43.0 in stage 2.0 (TID 33) in 47 ms on localhost (executor driver) (15/200)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 40.0 in stage 2.0 (TID 32) in 51 ms on localhost (executor driver) (16/200)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 60.0 in stage 2.0 (TID 41, localhost, executor driver, partition 60, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 49.0 in stage 2.0 (TID 36). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000046_34\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 60.0 in stage 2.0 (TID 41)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 48.0 in stage 2.0 (TID 35). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 62.0 in stage 2.0 (TID 42, localhost, executor driver, partition 62, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000029_27\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 46.0 in stage 2.0 (TID 34). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 28.0 in stage 2.0 (TID 26) in 57 ms on localhost (executor driver) (17/200)\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 27.0 in stage 2.0 (TID 25). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 49.0 in stage 2.0 (TID 36) in 50 ms on localhost (executor driver) (18/200)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000031_29\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 62.0 in stage 2.0 (TID 42)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 29.0 in stage 2.0 (TID 27). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 73.0 in stage 2.0 (TID 43, localhost, executor driver, partition 73, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 77.0 in stage 2.0 (TID 44, localhost, executor driver, partition 77, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 48.0 in stage 2.0 (TID 35) in 53 ms on localhost (executor driver) (19/200)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 46.0 in stage 2.0 (TID 34) in 53 ms on localhost (executor driver) (20/200)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 82.0 in stage 2.0 (TID 45, localhost, executor driver, partition 82, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 31.0 in stage 2.0 (TID 29). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 84.0 in stage 2.0 (TID 46, localhost, executor driver, partition 84, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 77.0 in stage 2.0 (TID 44)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 84.0 in stage 2.0 (TID 46)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 73.0 in stage 2.0 (TID 43)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 27.0 in stage 2.0 (TID 25) in 61 ms on localhost (executor driver) (21/200)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 29.0 in stage 2.0 (TID 27) in 59 ms on localhost (executor driver) (22/200)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 85.0 in stage 2.0 (TID 47, localhost, executor driver, partition 85, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 31.0 in stage 2.0 (TID 29) in 59 ms on localhost (executor driver) (23/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 82.0 in stage 2.0 (TID 45)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 85.0 in stage 2.0 (TID 47)\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000030_28\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 30.0 in stage 2.0 (TID 28). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 87.0 in stage 2.0 (TID 48, localhost, executor driver, partition 87, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 87.0 in stage 2.0 (TID 48)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 30.0 in stage 2.0 (TID 28) in 70 ms on localhost (executor driver) (24/200)\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000052_37\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:29 INFO Executor: Finished task 52.0 in stage 2.0 (TID 37). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 88.0 in stage 2.0 (TID 49, localhost, executor driver, partition 88, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 88.0 in stage 2.0 (TID 49)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 52.0 in stage 2.0 (TID 37) in 38 ms on localhost (executor driver) (25/200)\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000057_40\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 57.0 in stage 2.0 (TID 40). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 90.0 in stage 2.0 (TID 50, localhost, executor driver, partition 90, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 90.0 in stage 2.0 (TID 50)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000060_41\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 60.0 in stage 2.0 (TID 41). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 57.0 in stage 2.0 (TID 40) in 41 ms on localhost (executor driver) (26/200)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 92.0 in stage 2.0 (TID 51, localhost, executor driver, partition 92, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 92.0 in stage 2.0 (TID 51)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 60.0 in stage 2.0 (TID 41) in 41 ms on localhost (executor driver) (27/200)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000056_39\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 56.0 in stage 2.0 (TID 39). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 94.0 in stage 2.0 (TID 52, localhost, executor driver, partition 94, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 94.0 in stage 2.0 (TID 52)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 56.0 in stage 2.0 (TID 39) in 47 ms on localhost (executor driver) (28/200)\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000062_42\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 62.0 in stage 2.0 (TID 42). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 95.0 in stage 2.0 (TID 53, localhost, executor driver, partition 95, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 95.0 in stage 2.0 (TID 53)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 62.0 in stage 2.0 (TID 42) in 44 ms on localhost (executor driver) (29/200)\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000054_38\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000084_46\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 54.0 in stage 2.0 (TID 38). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 96.0 in stage 2.0 (TID 54, localhost, executor driver, partition 96, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 54.0 in stage 2.0 (TID 38) in 62 ms on localhost (executor driver) (30/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 96.0 in stage 2.0 (TID 54)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000085_47\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 84.0 in stage 2.0 (TID 46). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 106.0 in stage 2.0 (TID 55, localhost, executor driver, partition 106, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 84.0 in stage 2.0 (TID 46) in 50 ms on localhost (executor driver) (31/200)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000082_45\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 106.0 in stage 2.0 (TID 55)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000077_44\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 85.0 in stage 2.0 (TID 47). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 107.0 in stage 2.0 (TID 56, localhost, executor driver, partition 107, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 85.0 in stage 2.0 (TID 47) in 53 ms on localhost (executor driver) (32/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 107.0 in stage 2.0 (TID 56)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000087_48\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 87.0 in stage 2.0 (TID 48). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 82.0 in stage 2.0 (TID 45). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 110.0 in stage 2.0 (TID 57, localhost, executor driver, partition 110, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 111.0 in stage 2.0 (TID 58, localhost, executor driver, partition 111, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 87.0 in stage 2.0 (TID 48) in 48 ms on localhost (executor driver) (33/200)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000088_49\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 110.0 in stage 2.0 (TID 57)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 82.0 in stage 2.0 (TID 45) in 60 ms on localhost (executor driver) (34/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 111.0 in stage 2.0 (TID 58)\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 88.0 in stage 2.0 (TID 49). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 114.0 in stage 2.0 (TID 59, localhost, executor driver, partition 114, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 114.0 in stage 2.0 (TID 59)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000073_43\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 77.0 in stage 2.0 (TID 44). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 88.0 in stage 2.0 (TID 49) in 39 ms on localhost (executor driver) (35/200)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 119.0 in stage 2.0 (TID 60, localhost, executor driver, partition 119, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000090_50\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 77.0 in stage 2.0 (TID 44) in 98 ms on localhost (executor driver) (36/200)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 90.0 in stage 2.0 (TID 50). 3734 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 120.0 in stage 2.0 (TID 61, localhost, executor driver, partition 120, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 120.0 in stage 2.0 (TID 61)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 90.0 in stage 2.0 (TID 50) in 66 ms on localhost (executor driver) (37/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 119.0 in stage 2.0 (TID 60)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 73.0 in stage 2.0 (TID 43). 3734 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 121.0 in stage 2.0 (TID 62, localhost, executor driver, partition 121, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 121.0 in stage 2.0 (TID 62)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 73.0 in stage 2.0 (TID 43) in 109 ms on localhost (executor driver) (38/200)\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000096_54\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000095_53\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 96.0 in stage 2.0 (TID 54). 3734 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 124.0 in stage 2.0 (TID 63, localhost, executor driver, partition 124, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 96.0 in stage 2.0 (TID 54) in 79 ms on localhost (executor driver) (39/200)\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 95.0 in stage 2.0 (TID 53). 3734 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000119_60\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 125.0 in stage 2.0 (TID 64, localhost, executor driver, partition 125, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000092_51\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 95.0 in stage 2.0 (TID 53) in 90 ms on localhost (executor driver) (40/200)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 119.0 in stage 2.0 (TID 60). 3691 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 92.0 in stage 2.0 (TID 51). 3734 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO Executor: Running task 125.0 in stage 2.0 (TID 64)\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000107_56\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 107.0 in stage 2.0 (TID 56). 3734 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000114_59\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000110_57\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000094_52\n",
      "19/11/16 09:43:29 INFO Executor: Running task 124.0 in stage 2.0 (TID 63)\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 110.0 in stage 2.0 (TID 57). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 114.0 in stage 2.0 (TID 59). 3734 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 126.0 in stage 2.0 (TID 65, localhost, executor driver, partition 126, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000106_55\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 128.0 in stage 2.0 (TID 66, localhost, executor driver, partition 128, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 126.0 in stage 2.0 (TID 65)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 94.0 in stage 2.0 (TID 52). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 119.0 in stage 2.0 (TID 60) in 69 ms on localhost (executor driver) (41/200)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 92.0 in stage 2.0 (TID 51) in 100 ms on localhost (executor driver) (42/200)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 132.0 in stage 2.0 (TID 67, localhost, executor driver, partition 132, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 136.0 in stage 2.0 (TID 68, localhost, executor driver, partition 136, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 107.0 in stage 2.0 (TID 56) in 82 ms on localhost (executor driver) (43/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 136.0 in stage 2.0 (TID 68)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 106.0 in stage 2.0 (TID 55). 3734 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 110.0 in stage 2.0 (TID 57) in 79 ms on localhost (executor driver) (44/200)\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 139.0 in stage 2.0 (TID 69, localhost, executor driver, partition 139, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO Executor: Running task 139.0 in stage 2.0 (TID 69)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 140.0 in stage 2.0 (TID 70, localhost, executor driver, partition 140, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 114.0 in stage 2.0 (TID 59) in 75 ms on localhost (executor driver) (45/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 140.0 in stage 2.0 (TID 70)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 94.0 in stage 2.0 (TID 52) in 101 ms on localhost (executor driver) (46/200)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 144.0 in stage 2.0 (TID 71, localhost, executor driver, partition 144, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 144.0 in stage 2.0 (TID 71)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 106.0 in stage 2.0 (TID 55) in 90 ms on localhost (executor driver) (47/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 128.0 in stage 2.0 (TID 66)\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO Executor: Running task 132.0 in stage 2.0 (TID 67)\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000121_62\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 121.0 in stage 2.0 (TID 62). 3734 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 145.0 in stage 2.0 (TID 72, localhost, executor driver, partition 145, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 145.0 in stage 2.0 (TID 72)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 121.0 in stage 2.0 (TID 62) in 36 ms on localhost (executor driver) (48/200)\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000111_58\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 111.0 in stage 2.0 (TID 58). 3734 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 147.0 in stage 2.0 (TID 73, localhost, executor driver, partition 147, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 111.0 in stage 2.0 (TID 58) in 89 ms on localhost (executor driver) (49/200)\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000120_61\n",
      "19/11/16 09:43:29 INFO Executor: Running task 147.0 in stage 2.0 (TID 73)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 120.0 in stage 2.0 (TID 61). 3691 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 150.0 in stage 2.0 (TID 74, localhost, executor driver, partition 150, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 120.0 in stage 2.0 (TID 61) in 54 ms on localhost (executor driver) (50/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 150.0 in stage 2.0 (TID 74)\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000124_63\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000126_65\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 124.0 in stage 2.0 (TID 63). 3691 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 152.0 in stage 2.0 (TID 75, localhost, executor driver, partition 152, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 126.0 in stage 2.0 (TID 65). 3691 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000136_68\n",
      "19/11/16 09:43:29 INFO Executor: Running task 152.0 in stage 2.0 (TID 75)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 153.0 in stage 2.0 (TID 76, localhost, executor driver, partition 153, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 124.0 in stage 2.0 (TID 63) in 42 ms on localhost (executor driver) (51/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 153.0 in stage 2.0 (TID 76)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 126.0 in stage 2.0 (TID 65) in 37 ms on localhost (executor driver) (52/200)\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 136.0 in stage 2.0 (TID 68). 3691 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 154.0 in stage 2.0 (TID 77, localhost, executor driver, partition 154, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 154.0 in stage 2.0 (TID 77)\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 136.0 in stage 2.0 (TID 68) in 32 ms on localhost (executor driver) (53/200)\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000140_70\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 140.0 in stage 2.0 (TID 70). 3691 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 156.0 in stage 2.0 (TID 78, localhost, executor driver, partition 156, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 140.0 in stage 2.0 (TID 70) in 33 ms on localhost (executor driver) (54/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 156.0 in stage 2.0 (TID 78)\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000144_71\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 144.0 in stage 2.0 (TID 71). 3691 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000139_69\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000145_72\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 159.0 in stage 2.0 (TID 79, localhost, executor driver, partition 159, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 144.0 in stage 2.0 (TID 71) in 35 ms on localhost (executor driver) (55/200)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 145.0 in stage 2.0 (TID 72). 3691 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 139.0 in stage 2.0 (TID 69). 3691 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Running task 159.0 in stage 2.0 (TID 79)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 160.0 in stage 2.0 (TID 80, localhost, executor driver, partition 160, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 161.0 in stage 2.0 (TID 81, localhost, executor driver, partition 161, PROCESS_LOCAL, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 160.0 in stage 2.0 (TID 80)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 145.0 in stage 2.0 (TID 72) in 32 ms on localhost (executor driver) (56/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 161.0 in stage 2.0 (TID 81)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 139.0 in stage 2.0 (TID 69) in 37 ms on localhost (executor driver) (57/200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000132_67\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 132.0 in stage 2.0 (TID 67). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 163.0 in stage 2.0 (TID 82, localhost, executor driver, partition 163, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 132.0 in stage 2.0 (TID 67) in 60 ms on localhost (executor driver) (58/200)\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000125_64\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 125.0 in stage 2.0 (TID 64). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 164.0 in stage 2.0 (TID 83, localhost, executor driver, partition 164, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 125.0 in stage 2.0 (TID 64) in 71 ms on localhost (executor driver) (59/200)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000150_74\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000152_75\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000156_78\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 163.0 in stage 2.0 (TID 82)\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 150.0 in stage 2.0 (TID 74). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 152.0 in stage 2.0 (TID 75). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 156.0 in stage 2.0 (TID 78). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 167.0 in stage 2.0 (TID 84, localhost, executor driver, partition 167, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 167.0 in stage 2.0 (TID 84)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 164.0 in stage 2.0 (TID 83)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 171.0 in stage 2.0 (TID 85, localhost, executor driver, partition 171, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 150.0 in stage 2.0 (TID 74) in 52 ms on localhost (executor driver) (60/200)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000153_76\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 152.0 in stage 2.0 (TID 75) in 36 ms on localhost (executor driver) (61/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 171.0 in stage 2.0 (TID 85)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000154_77\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 175.0 in stage 2.0 (TID 86, localhost, executor driver, partition 175, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 153.0 in stage 2.0 (TID 76). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 156.0 in stage 2.0 (TID 78) in 33 ms on localhost (executor driver) (62/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 154.0 in stage 2.0 (TID 77). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000159_79\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 177.0 in stage 2.0 (TID 87, localhost, executor driver, partition 177, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000147_73\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000161_81\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 178.0 in stage 2.0 (TID 88, localhost, executor driver, partition 178, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 177.0 in stage 2.0 (TID 87)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 153.0 in stage 2.0 (TID 76) in 38 ms on localhost (executor driver) (63/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 178.0 in stage 2.0 (TID 88)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 154.0 in stage 2.0 (TID 77) in 37 ms on localhost (executor driver) (64/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 159.0 in stage 2.0 (TID 79). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 147.0 in stage 2.0 (TID 73). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 179.0 in stage 2.0 (TID 89, localhost, executor driver, partition 179, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 161.0 in stage 2.0 (TID 81). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 179.0 in stage 2.0 (TID 89)\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 180.0 in stage 2.0 (TID 90, localhost, executor driver, partition 180, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 159.0 in stage 2.0 (TID 79) in 34 ms on localhost (executor driver) (65/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 180.0 in stage 2.0 (TID 90)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 182.0 in stage 2.0 (TID 91, localhost, executor driver, partition 182, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000160_80\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 147.0 in stage 2.0 (TID 73) in 60 ms on localhost (executor driver) (66/200)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 161.0 in stage 2.0 (TID 81) in 33 ms on localhost (executor driver) (67/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 160.0 in stage 2.0 (TID 80). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 186.0 in stage 2.0 (TID 92, localhost, executor driver, partition 186, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 186.0 in stage 2.0 (TID 92)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 160.0 in stage 2.0 (TID 80) in 35 ms on localhost (executor driver) (68/200)\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 175.0 in stage 2.0 (TID 86)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000128_66\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 128.0 in stage 2.0 (TID 66). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 189.0 in stage 2.0 (TID 93, localhost, executor driver, partition 189, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 128.0 in stage 2.0 (TID 66) in 76 ms on localhost (executor driver) (69/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 182.0 in stage 2.0 (TID 91)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 189.0 in stage 2.0 (TID 93)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000167_84\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000177_87\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000179_89\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 167.0 in stage 2.0 (TID 84). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000180_90\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000171_85\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 190.0 in stage 2.0 (TID 94, localhost, executor driver, partition 190, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 167.0 in stage 2.0 (TID 84) in 36 ms on localhost (executor driver) (70/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 180.0 in stage 2.0 (TID 90). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 193.0 in stage 2.0 (TID 95, localhost, executor driver, partition 193, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 193.0 in stage 2.0 (TID 95)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 180.0 in stage 2.0 (TID 90) in 33 ms on localhost (executor driver) (71/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 177.0 in stage 2.0 (TID 87). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 195.0 in stage 2.0 (TID 96, localhost, executor driver, partition 195, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 177.0 in stage 2.0 (TID 87) in 37 ms on localhost (executor driver) (72/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 179.0 in stage 2.0 (TID 89). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 196.0 in stage 2.0 (TID 97, localhost, executor driver, partition 196, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 190.0 in stage 2.0 (TID 94)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 179.0 in stage 2.0 (TID 89) in 37 ms on localhost (executor driver) (73/200)\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 195.0 in stage 2.0 (TID 96)\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 196.0 in stage 2.0 (TID 97)\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000178_88\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000186_92\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 186.0 in stage 2.0 (TID 92). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 171.0 in stage 2.0 (TID 85). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 178.0 in stage 2.0 (TID 88). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 198.0 in stage 2.0 (TID 98, localhost, executor driver, partition 198, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 198.0 in stage 2.0 (TID 98)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 199.0 in stage 2.0 (TID 99, localhost, executor driver, partition 199, PROCESS_LOCAL, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 186.0 in stage 2.0 (TID 92) in 41 ms on localhost (executor driver) (74/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 199.0 in stage 2.0 (TID 99)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 100, localhost, executor driver, partition 0, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 171.0 in stage 2.0 (TID 85) in 48 ms on localhost (executor driver) (75/200)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 178.0 in stage 2.0 (TID 88) in 45 ms on localhost (executor driver) (76/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 0.0 in stage 2.0 (TID 100)\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000163_82\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000189_93\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 163.0 in stage 2.0 (TID 82). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 189.0 in stage 2.0 (TID 93). 3691 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 101, localhost, executor driver, partition 1, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 1.0 in stage 2.0 (TID 101)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 102, localhost, executor driver, partition 2, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 163.0 in stage 2.0 (TID 82) in 59 ms on localhost (executor driver) (77/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 2.0 in stage 2.0 (TID 102)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 189.0 in stage 2.0 (TID 93) in 44 ms on localhost (executor driver) (78/200)\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000175_86\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 175.0 in stage 2.0 (TID 86). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 103, localhost, executor driver, partition 3, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 175.0 in stage 2.0 (TID 86) in 95 ms on localhost (executor driver) (79/200)\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 3.0 in stage 2.0 (TID 103)\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000164_83\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 164.0 in stage 2.0 (TID 83). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 104, localhost, executor driver, partition 4, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 4.0 in stage 2.0 (TID 104)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 164.0 in stage 2.0 (TID 83) in 106 ms on localhost (executor driver) (80/200)\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000182_91\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 182.0 in stage 2.0 (TID 91). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 105, localhost, executor driver, partition 6, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 6.0 in stage 2.0 (TID 105)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 182.0 in stage 2.0 (TID 91) in 106 ms on localhost (executor driver) (81/200)\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 5 non-empty blocks including 5 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000196_97\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 196.0 in stage 2.0 (TID 97). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000193_95\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 106, localhost, executor driver, partition 7, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 7.0 in stage 2.0 (TID 106)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 193.0 in stage 2.0 (TID 95). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 196.0 in stage 2.0 (TID 97) in 80 ms on localhost (executor driver) (82/200)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 107, localhost, executor driver, partition 8, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 8.0 in stage 2.0 (TID 107)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 193.0 in stage 2.0 (TID 95) in 83 ms on localhost (executor driver) (83/200)\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000190_94\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 190.0 in stage 2.0 (TID 94). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 108, localhost, executor driver, partition 9, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 190.0 in stage 2.0 (TID 94) in 90 ms on localhost (executor driver) (84/200)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000198_98\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 9.0 in stage 2.0 (TID 108)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 198.0 in stage 2.0 (TID 98). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 109, localhost, executor driver, partition 11, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 11.0 in stage 2.0 (TID 109)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 198.0 in stage 2.0 (TID 98) in 81 ms on localhost (executor driver) (85/200)\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000195_96\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20191116094327_0002_m_000199_99\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 195.0 in stage 2.0 (TID 96). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 110, localhost, executor driver, partition 12, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 12.0 in stage 2.0 (TID 110)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 199.0 in stage 2.0 (TID 99). 3734 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 195.0 in stage 2.0 (TID 96) in 91 ms on localhost (executor driver) (86/200)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 14.0 in stage 2.0 (TID 111, localhost, executor driver, partition 14, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 14.0 in stage 2.0 (TID 111)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 199.0 in stage 2.0 (TID 99) in 83 ms on localhost (executor driver) (87/200)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 7 non-empty blocks including 7 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 11 non-empty blocks including 11 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks including 8 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000007_106' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000007\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000006_105' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000006\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000002_102' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000002\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000003_103' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000003\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000004_104' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000004\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000000_100' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000000\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000001_101' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000001\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000001_101: Committed\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000004_104: Committed\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000003_103: Committed\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000007_106: Committed\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000000_100: Committed\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000006_105: Committed\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000002_102: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000012_110' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000012\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000012_110: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000014_111' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000014\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000014_111: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000009_108' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000009\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000009_108: Committed\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 4.0 in stage 2.0 (TID 104). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 0.0 in stage 2.0 (TID 100). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 7.0 in stage 2.0 (TID 106). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000008_107' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000008\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000008_107: Committed\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 15.0 in stage 2.0 (TID 112, localhost, executor driver, partition 15, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000011_109' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000011\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 3.0 in stage 2.0 (TID 103). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000011_109: Committed\n",
      "19/11/16 09:43:29 INFO Executor: Running task 15.0 in stage 2.0 (TID 112)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 18.0 in stage 2.0 (TID 113, localhost, executor driver, partition 18, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 104) in 174 ms on localhost (executor driver) (88/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 18.0 in stage 2.0 (TID 113)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 2.0 in stage 2.0 (TID 102). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 1.0 in stage 2.0 (TID 101). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 6.0 in stage 2.0 (TID 105). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 21.0 in stage 2.0 (TID 114, localhost, executor driver, partition 21, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 11.0 in stage 2.0 (TID 109). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 100) in 229 ms on localhost (executor driver) (89/200)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 12.0 in stage 2.0 (TID 110). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 8.0 in stage 2.0 (TID 107). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Running task 21.0 in stage 2.0 (TID 114)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 32.0 in stage 2.0 (TID 115, localhost, executor driver, partition 32, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 106) in 159 ms on localhost (executor driver) (90/200)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 9.0 in stage 2.0 (TID 108). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Running task 32.0 in stage 2.0 (TID 115)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 34.0 in stage 2.0 (TID 116, localhost, executor driver, partition 34, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 14.0 in stage 2.0 (TID 111). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 103) in 183 ms on localhost (executor driver) (91/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 34.0 in stage 2.0 (TID 116)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 35.0 in stage 2.0 (TID 117, localhost, executor driver, partition 35, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 102) in 227 ms on localhost (executor driver) (92/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 35.0 in stage 2.0 (TID 117)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 36.0 in stage 2.0 (TID 118, localhost, executor driver, partition 36, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 101) in 228 ms on localhost (executor driver) (93/200)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 37.0 in stage 2.0 (TID 119, localhost, executor driver, partition 37, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 105) in 170 ms on localhost (executor driver) (94/200)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 38.0 in stage 2.0 (TID 120, localhost, executor driver, partition 38, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 109) in 154 ms on localhost (executor driver) (95/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 38.0 in stage 2.0 (TID 120)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 41.0 in stage 2.0 (TID 121, localhost, executor driver, partition 41, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 110) in 153 ms on localhost (executor driver) (96/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 41.0 in stage 2.0 (TID 121)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 42.0 in stage 2.0 (TID 122, localhost, executor driver, partition 42, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 107) in 161 ms on localhost (executor driver) (97/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 42.0 in stage 2.0 (TID 122)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 44.0 in stage 2.0 (TID 123, localhost, executor driver, partition 44, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 108) in 158 ms on localhost (executor driver) (98/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 44.0 in stage 2.0 (TID 123)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 14.0 in stage 2.0 (TID 111) in 153 ms on localhost (executor driver) (99/200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:29 INFO Executor: Running task 36.0 in stage 2.0 (TID 118)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 37.0 in stage 2.0 (TID 119)\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 7 non-empty blocks including 7 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 5 non-empty blocks including 5 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 9 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 6 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 7 non-empty blocks including 7 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000018_113' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000018\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000018_113: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000034_116' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000034\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000034_116: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000042_122' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000042\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000042_122: Committed\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 18.0 in stage 2.0 (TID 113). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000015_112' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000015\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000015_112: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000038_120' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000038\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000038_120: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000044_123' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000044\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000044_123: Committed\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 45.0 in stage 2.0 (TID 124, localhost, executor driver, partition 45, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 34.0 in stage 2.0 (TID 116). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 42.0 in stage 2.0 (TID 122). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Running task 45.0 in stage 2.0 (TID 124)\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000037_119' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000037\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 47.0 in stage 2.0 (TID 125, localhost, executor driver, partition 47, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 18.0 in stage 2.0 (TID 113) in 110 ms on localhost (executor driver) (100/200)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 38.0 in stage 2.0 (TID 120). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Running task 47.0 in stage 2.0 (TID 125)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 44.0 in stage 2.0 (TID 123). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 50.0 in stage 2.0 (TID 126, localhost, executor driver, partition 50, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 34.0 in stage 2.0 (TID 116) in 109 ms on localhost (executor driver) (101/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 50.0 in stage 2.0 (TID 126)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 51.0 in stage 2.0 (TID 127, localhost, executor driver, partition 51, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 15.0 in stage 2.0 (TID 112). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Running task 51.0 in stage 2.0 (TID 127)\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000037_119: Committed\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 42.0 in stage 2.0 (TID 122) in 106 ms on localhost (executor driver) (102/200)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 38.0 in stage 2.0 (TID 120) in 107 ms on localhost (executor driver) (103/200)\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000035_117' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000035\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 53.0 in stage 2.0 (TID 128, localhost, executor driver, partition 53, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 55.0 in stage 2.0 (TID 129, localhost, executor driver, partition 55, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 44.0 in stage 2.0 (TID 123) in 106 ms on localhost (executor driver) (104/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 55.0 in stage 2.0 (TID 129)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 15.0 in stage 2.0 (TID 112) in 112 ms on localhost (executor driver) (105/200)\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000035_117: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000041_121' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000041\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000041_121: Committed\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 35.0 in stage 2.0 (TID 117). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 58.0 in stage 2.0 (TID 130, localhost, executor driver, partition 58, ANY, 7767 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:29 INFO Executor: Finished task 41.0 in stage 2.0 (TID 121). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 35.0 in stage 2.0 (TID 117) in 111 ms on localhost (executor driver) (106/200)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 37.0 in stage 2.0 (TID 119). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 59.0 in stage 2.0 (TID 131, localhost, executor driver, partition 59, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 61.0 in stage 2.0 (TID 132, localhost, executor driver, partition 61, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 41.0 in stage 2.0 (TID 121) in 110 ms on localhost (executor driver) (107/200)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 37.0 in stage 2.0 (TID 119) in 111 ms on localhost (executor driver) (108/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 53.0 in stage 2.0 (TID 128)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 59.0 in stage 2.0 (TID 131)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 58.0 in stage 2.0 (TID 130)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 61.0 in stage 2.0 (TID 132)\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000036_118' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000036\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000036_118: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000032_115' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000032\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000032_115: Committed\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 36.0 in stage 2.0 (TID 118). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 63.0 in stage 2.0 (TID 133, localhost, executor driver, partition 63, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 36.0 in stage 2.0 (TID 118) in 116 ms on localhost (executor driver) (109/200)\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000021_114' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000021\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000021_114: Committed\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 32.0 in stage 2.0 (TID 115). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Running task 63.0 in stage 2.0 (TID 133)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 64.0 in stage 2.0 (TID 134, localhost, executor driver, partition 64, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 21.0 in stage 2.0 (TID 114). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Running task 64.0 in stage 2.0 (TID 134)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 65.0 in stage 2.0 (TID 135, localhost, executor driver, partition 65, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 32.0 in stage 2.0 (TID 115) in 119 ms on localhost (executor driver) (110/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 65.0 in stage 2.0 (TID 135)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 21.0 in stage 2.0 (TID 114) in 121 ms on localhost (executor driver) (111/200)\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 6 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 12 non-empty blocks including 12 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000055_129' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000055\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000055_129: Committed\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 55.0 in stage 2.0 (TID 129). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 66.0 in stage 2.0 (TID 136, localhost, executor driver, partition 66, ANY, 7767 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:29 INFO Executor: Running task 66.0 in stage 2.0 (TID 136)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 55.0 in stage 2.0 (TID 129) in 105 ms on localhost (executor driver) (112/200)\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000063_133' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000063\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000063_133: Committed\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 63.0 in stage 2.0 (TID 133). 3820 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 67.0 in stage 2.0 (TID 137, localhost, executor driver, partition 67, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 63.0 in stage 2.0 (TID 133) in 112 ms on localhost (executor driver) (113/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 67.0 in stage 2.0 (TID 137)\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000050_126' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000050\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000050_126: Committed\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000047_125' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000047\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000047_125: Committed\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 50.0 in stage 2.0 (TID 126). 3820 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 47.0 in stage 2.0 (TID 125). 3820 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 68.0 in stage 2.0 (TID 138, localhost, executor driver, partition 68, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 69.0 in stage 2.0 (TID 139, localhost, executor driver, partition 69, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000059_131' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000059\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 50.0 in stage 2.0 (TID 126) in 128 ms on localhost (executor driver) (114/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 68.0 in stage 2.0 (TID 138)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 47.0 in stage 2.0 (TID 125) in 128 ms on localhost (executor driver) (115/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 69.0 in stage 2.0 (TID 139)\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000059_131: Committed\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 59.0 in stage 2.0 (TID 131). 3820 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000064_134' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000064\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000064_134: Committed\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 70.0 in stage 2.0 (TID 140, localhost, executor driver, partition 70, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000053_128' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000053\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000053_128: Committed\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 70.0 in stage 2.0 (TID 140)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 59.0 in stage 2.0 (TID 131) in 126 ms on localhost (executor driver) (116/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 64.0 in stage 2.0 (TID 134). 3820 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 53.0 in stage 2.0 (TID 128). 3820 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 71.0 in stage 2.0 (TID 141, localhost, executor driver, partition 71, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 71.0 in stage 2.0 (TID 141)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 72.0 in stage 2.0 (TID 142, localhost, executor driver, partition 72, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 64.0 in stage 2.0 (TID 134) in 121 ms on localhost (executor driver) (117/200)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 53.0 in stage 2.0 (TID 128) in 130 ms on localhost (executor driver) (118/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 72.0 in stage 2.0 (TID 142)\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000061_132' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000061\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000061_132: Committed\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000045_124' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000045\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000045_124: Committed\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 61.0 in stage 2.0 (TID 132). 3820 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 74.0 in stage 2.0 (TID 143, localhost, executor driver, partition 74, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 45.0 in stage 2.0 (TID 124). 3820 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 74.0 in stage 2.0 (TID 143)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 61.0 in stage 2.0 (TID 132) in 131 ms on localhost (executor driver) (119/200)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 75.0 in stage 2.0 (TID 144, localhost, executor driver, partition 75, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 45.0 in stage 2.0 (TID 124) in 137 ms on localhost (executor driver) (120/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 75.0 in stage 2.0 (TID 144)\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000051_127' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000051\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000051_127: Committed\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 51.0 in stage 2.0 (TID 127). 3820 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000058_130' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000058\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000058_130: Committed\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 76.0 in stage 2.0 (TID 145, localhost, executor driver, partition 76, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 51.0 in stage 2.0 (TID 127) in 137 ms on localhost (executor driver) (121/200)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 76.0 in stage 2.0 (TID 145)\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 58.0 in stage 2.0 (TID 130). 3820 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 78.0 in stage 2.0 (TID 146, localhost, executor driver, partition 78, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 78.0 in stage 2.0 (TID 146)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 58.0 in stage 2.0 (TID 130) in 135 ms on localhost (executor driver) (122/200)\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000065_135' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000065\r\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000065_135: Committed\r\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 65.0 in stage 2.0 (TID 135). 3820 bytes result sent to driver\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 79.0 in stage 2.0 (TID 147, localhost, executor driver, partition 79, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:29 INFO Executor: Running task 79.0 in stage 2.0 (TID 147)\r\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 65.0 in stage 2.0 (TID 135) in 130 ms on localhost (executor driver) (123/200)\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 9 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 9 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 9 non-empty blocks including 9 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000067_137' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000067\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000067_137: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000072_142' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000072\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000072_142: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000066_136' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000066\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000066_136: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000068_138' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000068\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000068_138: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000069_139' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000069\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000069_139: Committed\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 67.0 in stage 2.0 (TID 137). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 72.0 in stage 2.0 (TID 142). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 69.0 in stage 2.0 (TID 139). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 80.0 in stage 2.0 (TID 148, localhost, executor driver, partition 80, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 81.0 in stage 2.0 (TID 149, localhost, executor driver, partition 81, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 66.0 in stage 2.0 (TID 136). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 67.0 in stage 2.0 (TID 137) in 100 ms on localhost (executor driver) (124/200)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 72.0 in stage 2.0 (TID 142) in 89 ms on localhost (executor driver) (125/200)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 83.0 in stage 2.0 (TID 150, localhost, executor driver, partition 83, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 81.0 in stage 2.0 (TID 149)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 80.0 in stage 2.0 (TID 148)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 83.0 in stage 2.0 (TID 150)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 86.0 in stage 2.0 (TID 151, localhost, executor driver, partition 86, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 68.0 in stage 2.0 (TID 138). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 69.0 in stage 2.0 (TID 139) in 95 ms on localhost (executor driver) (126/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 86.0 in stage 2.0 (TID 151)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 66.0 in stage 2.0 (TID 136) in 116 ms on localhost (executor driver) (127/200)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 89.0 in stage 2.0 (TID 152, localhost, executor driver, partition 89, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 68.0 in stage 2.0 (TID 138) in 95 ms on localhost (executor driver) (128/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 89.0 in stage 2.0 (TID 152)\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000070_140' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000070\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000070_140: Committed\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 70.0 in stage 2.0 (TID 140). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000075_144' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000075\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 91.0 in stage 2.0 (TID 153, localhost, executor driver, partition 91, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 70.0 in stage 2.0 (TID 140) in 96 ms on localhost (executor driver) (129/200)\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000075_144: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000071_141' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000071\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000071_141: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000079_147' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000079\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000079_147: Committed\n",
      "19/11/16 09:43:29 INFO Executor: Running task 91.0 in stage 2.0 (TID 153)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 79.0 in stage 2.0 (TID 147). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 75.0 in stage 2.0 (TID 144). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 93.0 in stage 2.0 (TID 154, localhost, executor driver, partition 93, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 93.0 in stage 2.0 (TID 154)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 97.0 in stage 2.0 (TID 155, localhost, executor driver, partition 97, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 79.0 in stage 2.0 (TID 147) in 89 ms on localhost (executor driver) (130/200)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 75.0 in stage 2.0 (TID 144) in 94 ms on localhost (executor driver) (131/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 97.0 in stage 2.0 (TID 155)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 71.0 in stage 2.0 (TID 141). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 98.0 in stage 2.0 (TID 156, localhost, executor driver, partition 98, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 71.0 in stage 2.0 (TID 141) in 100 ms on localhost (executor driver) (132/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 98.0 in stage 2.0 (TID 156)\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000078_146' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000078\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000078_146: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000076_145' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000076\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000076_145: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000074_143' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000074\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000074_143: Committed\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 78.0 in stage 2.0 (TID 146). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 76.0 in stage 2.0 (TID 145). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 74.0 in stage 2.0 (TID 143). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 99.0 in stage 2.0 (TID 157, localhost, executor driver, partition 99, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 99.0 in stage 2.0 (TID 157)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 100.0 in stage 2.0 (TID 158, localhost, executor driver, partition 100, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 78.0 in stage 2.0 (TID 146) in 100 ms on localhost (executor driver) (133/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 100.0 in stage 2.0 (TID 158)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 101.0 in stage 2.0 (TID 159, localhost, executor driver, partition 101, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 76.0 in stage 2.0 (TID 145) in 102 ms on localhost (executor driver) (134/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 101.0 in stage 2.0 (TID 159)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 74.0 in stage 2.0 (TID 143) in 105 ms on localhost (executor driver) (135/200)\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 7 non-empty blocks including 7 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 11 non-empty blocks including 11 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 5 non-empty blocks including 5 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 6 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 6 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000083_150' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000083\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000083_150: Committed\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 83.0 in stage 2.0 (TID 150). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 102.0 in stage 2.0 (TID 160, localhost, executor driver, partition 102, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 102.0 in stage 2.0 (TID 160)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 83.0 in stage 2.0 (TID 150) in 68 ms on localhost (executor driver) (136/200)\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000089_152' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000089\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000089_152: Committed\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 89.0 in stage 2.0 (TID 152). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 103.0 in stage 2.0 (TID 161, localhost, executor driver, partition 103, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 103.0 in stage 2.0 (TID 161)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 89.0 in stage 2.0 (TID 152) in 74 ms on localhost (executor driver) (137/200)\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000098_156' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000098\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000098_156: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000100_158' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000100\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000100_158: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000081_149' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000081\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000081_149: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000093_154' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000093\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000093_154: Committed\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 98.0 in stage 2.0 (TID 156). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 93.0 in stage 2.0 (TID 154). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 100.0 in stage 2.0 (TID 158). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 81.0 in stage 2.0 (TID 149). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 104.0 in stage 2.0 (TID 162, localhost, executor driver, partition 104, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 104.0 in stage 2.0 (TID 162)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 105.0 in stage 2.0 (TID 163, localhost, executor driver, partition 105, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 98.0 in stage 2.0 (TID 156) in 85 ms on localhost (executor driver) (138/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 105.0 in stage 2.0 (TID 163)\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 6 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 108.0 in stage 2.0 (TID 164, localhost, executor driver, partition 108, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 93.0 in stage 2.0 (TID 154) in 87 ms on localhost (executor driver) (139/200)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 109.0 in stage 2.0 (TID 165, localhost, executor driver, partition 109, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 100.0 in stage 2.0 (TID 158) in 77 ms on localhost (executor driver) (140/200)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 81.0 in stage 2.0 (TID 149) in 96 ms on localhost (executor driver) (141/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 109.0 in stage 2.0 (TID 165)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 108.0 in stage 2.0 (TID 164)\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000091_153' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000091\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000091_153: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000086_151' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000086\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000086_151: Committed\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 91.0 in stage 2.0 (TID 153). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000101_159' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000101\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000101_159: Committed\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 112.0 in stage 2.0 (TID 166, localhost, executor driver, partition 112, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 112.0 in stage 2.0 (TID 166)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 91.0 in stage 2.0 (TID 153) in 93 ms on localhost (executor driver) (142/200)\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 86.0 in stage 2.0 (TID 151). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 101.0 in stage 2.0 (TID 159). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 113.0 in stage 2.0 (TID 167, localhost, executor driver, partition 113, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 113.0 in stage 2.0 (TID 167)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 115.0 in stage 2.0 (TID 168, localhost, executor driver, partition 115, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 86.0 in stage 2.0 (TID 151) in 99 ms on localhost (executor driver) (143/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 115.0 in stage 2.0 (TID 168)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 101.0 in stage 2.0 (TID 159) in 81 ms on localhost (executor driver) (144/200)\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000099_157' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000099\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000099_157: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000097_155' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000097\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000097_155: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000080_148' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000080\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000080_148: Committed\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 99.0 in stage 2.0 (TID 157). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 97.0 in stage 2.0 (TID 155). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 80.0 in stage 2.0 (TID 148). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 116.0 in stage 2.0 (TID 169, localhost, executor driver, partition 116, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 116.0 in stage 2.0 (TID 169)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 117.0 in stage 2.0 (TID 170, localhost, executor driver, partition 117, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 99.0 in stage 2.0 (TID 157) in 91 ms on localhost (executor driver) (145/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 117.0 in stage 2.0 (TID 170)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 118.0 in stage 2.0 (TID 171, localhost, executor driver, partition 118, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 97.0 in stage 2.0 (TID 155) in 101 ms on localhost (executor driver) (146/200)\n",
      "19/11/16 09:43:29 INFO Executor: Running task 118.0 in stage 2.0 (TID 171)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 80.0 in stage 2.0 (TID 148) in 110 ms on localhost (executor driver) (147/200)\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 11 non-empty blocks including 11 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 5 non-empty blocks including 5 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 8 non-empty blocks including 8 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000102_160' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000102\n",
      "19/11/16 09:43:29 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000102_160: Committed\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:29 INFO Executor: Finished task 102.0 in stage 2.0 (TID 160). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Starting task 122.0 in stage 2.0 (TID 172, localhost, executor driver, partition 122, ANY, 7767 bytes)\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO Executor: Running task 122.0 in stage 2.0 (TID 172)\n",
      "19/11/16 09:43:29 INFO TaskSetManager: Finished task 102.0 in stage 2.0 (TID 160) in 100 ms on localhost (executor driver) (148/200)\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000103_161' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000103\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000103_161: Committed\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 103.0 in stage 2.0 (TID 161). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 123.0 in stage 2.0 (TID 173, localhost, executor driver, partition 123, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 123.0 in stage 2.0 (TID 173)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 103.0 in stage 2.0 (TID 161) in 117 ms on localhost (executor driver) (149/200)\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000115_168' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000115\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000115_168: Committed\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000117_170' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000117\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000117_170: Committed\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 115.0 in stage 2.0 (TID 168). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 117.0 in stage 2.0 (TID 170). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 127.0 in stage 2.0 (TID 174, localhost, executor driver, partition 127, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO Executor: Running task 127.0 in stage 2.0 (TID 174)\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 129.0 in stage 2.0 (TID 175, localhost, executor driver, partition 129, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 115.0 in stage 2.0 (TID 168) in 110 ms on localhost (executor driver) (150/200)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 129.0 in stage 2.0 (TID 175)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 117.0 in stage 2.0 (TID 170) in 100 ms on localhost (executor driver) (151/200)\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000118_171' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000118\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000118_171: Committed\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000116_169' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000116\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000116_169: Committed\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 118.0 in stage 2.0 (TID 171). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 116.0 in stage 2.0 (TID 169). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 130.0 in stage 2.0 (TID 176, localhost, executor driver, partition 130, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 130.0 in stage 2.0 (TID 176)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 131.0 in stage 2.0 (TID 177, localhost, executor driver, partition 131, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 118.0 in stage 2.0 (TID 171) in 102 ms on localhost (executor driver) (152/200)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 131.0 in stage 2.0 (TID 177)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 116.0 in stage 2.0 (TID 169) in 104 ms on localhost (executor driver) (153/200)\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000109_165' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000109\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000109_165: Committed\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000104_162' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000104\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000104_162: Committed\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000108_164' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000108\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000108_164: Committed\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000112_166' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000112\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000112_166: Committed\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 109.0 in stage 2.0 (TID 165). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 104.0 in stage 2.0 (TID 162). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000105_163' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000105\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000105_163: Committed\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 108.0 in stage 2.0 (TID 164). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 112.0 in stage 2.0 (TID 166). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 133.0 in stage 2.0 (TID 178, localhost, executor driver, partition 133, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 105.0 in stage 2.0 (TID 163). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO Executor: Running task 133.0 in stage 2.0 (TID 178)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 134.0 in stage 2.0 (TID 179, localhost, executor driver, partition 134, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 109.0 in stage 2.0 (TID 165) in 124 ms on localhost (executor driver) (154/200)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 135.0 in stage 2.0 (TID 180, localhost, executor driver, partition 135, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 104.0 in stage 2.0 (TID 162) in 127 ms on localhost (executor driver) (155/200)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 134.0 in stage 2.0 (TID 179)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 137.0 in stage 2.0 (TID 181, localhost, executor driver, partition 137, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 108.0 in stage 2.0 (TID 164) in 127 ms on localhost (executor driver) (156/200)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 112.0 in stage 2.0 (TID 166) in 124 ms on localhost (executor driver) (157/200)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 135.0 in stage 2.0 (TID 180)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 138.0 in stage 2.0 (TID 182, localhost, executor driver, partition 138, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 138.0 in stage 2.0 (TID 182)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 105.0 in stage 2.0 (TID 163) in 128 ms on localhost (executor driver) (158/200)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 137.0 in stage 2.0 (TID 181)\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 6 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000113_167' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000113\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000113_167: Committed\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000122_172' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000122\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000122_172: Committed\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 113.0 in stage 2.0 (TID 167). 3820 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 122.0 in stage 2.0 (TID 172). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 141.0 in stage 2.0 (TID 183, localhost, executor driver, partition 141, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 141.0 in stage 2.0 (TID 183)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 142.0 in stage 2.0 (TID 184, localhost, executor driver, partition 142, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 113.0 in stage 2.0 (TID 167) in 152 ms on localhost (executor driver) (159/200)\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 142.0 in stage 2.0 (TID 184)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 122.0 in stage 2.0 (TID 172) in 83 ms on localhost (executor driver) (160/200)\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000123_173' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000123\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000123_173: Committed\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000127_174' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000127\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000127_174: Committed\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 123.0 in stage 2.0 (TID 173). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 127.0 in stage 2.0 (TID 174). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 143.0 in stage 2.0 (TID 185, localhost, executor driver, partition 143, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000129_175' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000129\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000129_175: Committed\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 143.0 in stage 2.0 (TID 185)\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000131_177' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000131\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000131_177: Committed\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 146.0 in stage 2.0 (TID 186, localhost, executor driver, partition 146, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 123.0 in stage 2.0 (TID 173) in 89 ms on localhost (executor driver) (161/200)\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 146.0 in stage 2.0 (TID 186)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 127.0 in stage 2.0 (TID 174) in 74 ms on localhost (executor driver) (162/200)\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 129.0 in stage 2.0 (TID 175). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 148.0 in stage 2.0 (TID 187, localhost, executor driver, partition 148, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 131.0 in stage 2.0 (TID 177). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 148.0 in stage 2.0 (TID 187)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 129.0 in stage 2.0 (TID 175) in 74 ms on localhost (executor driver) (163/200)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 149.0 in stage 2.0 (TID 188, localhost, executor driver, partition 149, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 149.0 in stage 2.0 (TID 188)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 131.0 in stage 2.0 (TID 177) in 71 ms on localhost (executor driver) (164/200)\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000138_182' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000138\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000138_182: Committed\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 138.0 in stage 2.0 (TID 182). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000133_178' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000133\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000133_178: Committed\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 151.0 in stage 2.0 (TID 189, localhost, executor driver, partition 151, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000130_176' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000130\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000130_176: Committed\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 133.0 in stage 2.0 (TID 178). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 138.0 in stage 2.0 (TID 182) in 69 ms on localhost (executor driver) (165/200)\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 151.0 in stage 2.0 (TID 189)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 155.0 in stage 2.0 (TID 190, localhost, executor driver, partition 155, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000137_181' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000137\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000137_181: Committed\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 133.0 in stage 2.0 (TID 178) in 73 ms on localhost (executor driver) (166/200)\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 130.0 in stage 2.0 (TID 176). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 157.0 in stage 2.0 (TID 191, localhost, executor driver, partition 157, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 155.0 in stage 2.0 (TID 190)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 130.0 in stage 2.0 (TID 176) in 80 ms on localhost (executor driver) (167/200)\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 137.0 in stage 2.0 (TID 181). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 157.0 in stage 2.0 (TID 191)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 158.0 in stage 2.0 (TID 192, localhost, executor driver, partition 158, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 137.0 in stage 2.0 (TID 181) in 72 ms on localhost (executor driver) (168/200)\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 158.0 in stage 2.0 (TID 192)\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 6 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 6 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 6 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 4 non-empty blocks including 4 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 5 non-empty blocks including 5 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000135_180' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000135\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000135_180: Committed\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000134_179' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000134\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000134_179: Committed\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 134.0 in stage 2.0 (TID 179). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 135.0 in stage 2.0 (TID 180). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 162.0 in stage 2.0 (TID 193, localhost, executor driver, partition 162, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 162.0 in stage 2.0 (TID 193)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 165.0 in stage 2.0 (TID 194, localhost, executor driver, partition 165, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 134.0 in stage 2.0 (TID 179) in 97 ms on localhost (executor driver) (169/200)\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 165.0 in stage 2.0 (TID 194)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 135.0 in stage 2.0 (TID 180) in 96 ms on localhost (executor driver) (170/200)\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000142_184' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000142\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000142_184: Committed\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000141_183' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000141\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000141_183: Committed\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 142.0 in stage 2.0 (TID 184). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 141.0 in stage 2.0 (TID 183). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 166.0 in stage 2.0 (TID 195, localhost, executor driver, partition 166, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 166.0 in stage 2.0 (TID 195)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 168.0 in stage 2.0 (TID 196, localhost, executor driver, partition 168, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 142.0 in stage 2.0 (TID 184) in 83 ms on localhost (executor driver) (171/200)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 141.0 in stage 2.0 (TID 183) in 84 ms on localhost (executor driver) (172/200)\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 168.0 in stage 2.0 (TID 196)\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\r\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000148_187' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000148\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000148_187: Committed\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000149_188' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000149\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000149_188: Committed\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000146_186' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000146\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000146_186: Committed\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 148.0 in stage 2.0 (TID 187). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 149.0 in stage 2.0 (TID 188). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 146.0 in stage 2.0 (TID 186). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 169.0 in stage 2.0 (TID 197, localhost, executor driver, partition 169, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 169.0 in stage 2.0 (TID 197)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 170.0 in stage 2.0 (TID 198, localhost, executor driver, partition 170, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 148.0 in stage 2.0 (TID 187) in 64 ms on localhost (executor driver) (173/200)\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 170.0 in stage 2.0 (TID 198)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 149.0 in stage 2.0 (TID 188) in 63 ms on localhost (executor driver) (174/200)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 172.0 in stage 2.0 (TID 199, localhost, executor driver, partition 172, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 172.0 in stage 2.0 (TID 199)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 146.0 in stage 2.0 (TID 186) in 64 ms on localhost (executor driver) (175/200)\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000143_185' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000143\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000143_185: Committed\r\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000151_189' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000151\r\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000151_189: Committed\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 7 non-empty blocks including 7 local blocks and 0 remote blocks\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 151.0 in stage 2.0 (TID 189). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 143.0 in stage 2.0 (TID 185). 3777 bytes result sent to driver\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 173.0 in stage 2.0 (TID 200, localhost, executor driver, partition 173, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO Executor: Running task 173.0 in stage 2.0 (TID 200)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 174.0 in stage 2.0 (TID 201, localhost, executor driver, partition 174, ANY, 7767 bytes)\r\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 151.0 in stage 2.0 (TID 189) in 61 ms on localhost (executor driver) (176/200)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 143.0 in stage 2.0 (TID 185) in 70 ms on localhost (executor driver) (177/200)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 174.0 in stage 2.0 (TID 201)\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000158_192' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000158\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000158_192: Committed\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000155_190' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000155\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000155_190: Committed\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 155.0 in stage 2.0 (TID 190). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 158.0 in stage 2.0 (TID 192). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 176.0 in stage 2.0 (TID 202, localhost, executor driver, partition 176, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 176.0 in stage 2.0 (TID 202)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 181.0 in stage 2.0 (TID 203, localhost, executor driver, partition 181, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 155.0 in stage 2.0 (TID 190) in 65 ms on localhost (executor driver) (178/200)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 158.0 in stage 2.0 (TID 192) in 64 ms on localhost (executor driver) (179/200)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 181.0 in stage 2.0 (TID 203)\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 7 non-empty blocks including 7 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks including 2 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 6 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 5 non-empty blocks including 5 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 7 non-empty blocks including 7 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 6 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000157_191' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000157\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000157_191: Committed\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 157.0 in stage 2.0 (TID 191). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 183.0 in stage 2.0 (TID 204, localhost, executor driver, partition 183, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 157.0 in stage 2.0 (TID 191) in 106 ms on localhost (executor driver) (180/200)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 183.0 in stage 2.0 (TID 204)\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000170_198' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000170\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000170_198: Committed\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000169_197' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000169\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000169_197: Committed\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 169.0 in stage 2.0 (TID 197). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 170.0 in stage 2.0 (TID 198). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 184.0 in stage 2.0 (TID 205, localhost, executor driver, partition 184, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 184.0 in stage 2.0 (TID 205)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 185.0 in stage 2.0 (TID 206, localhost, executor driver, partition 185, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 169.0 in stage 2.0 (TID 197) in 60 ms on localhost (executor driver) (181/200)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 185.0 in stage 2.0 (TID 206)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 170.0 in stage 2.0 (TID 198) in 60 ms on localhost (executor driver) (182/200)\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000165_194' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000165\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000165_194: Committed\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000162_193' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000162\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000162_193: Committed\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000168_196' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000168\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000168_196: Committed\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 165.0 in stage 2.0 (TID 194). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 162.0 in stage 2.0 (TID 193). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 168.0 in stage 2.0 (TID 196). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 187.0 in stage 2.0 (TID 207, localhost, executor driver, partition 187, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 187.0 in stage 2.0 (TID 207)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 188.0 in stage 2.0 (TID 208, localhost, executor driver, partition 188, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 165.0 in stage 2.0 (TID 194) in 94 ms on localhost (executor driver) (183/200)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 162.0 in stage 2.0 (TID 193) in 94 ms on localhost (executor driver) (184/200)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 191.0 in stage 2.0 (TID 209, localhost, executor driver, partition 191, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 168.0 in stage 2.0 (TID 196) in 78 ms on localhost (executor driver) (185/200)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 191.0 in stage 2.0 (TID 209)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 188.0 in stage 2.0 (TID 208)\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000174_201' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000174\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000174_201: Committed\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 174.0 in stage 2.0 (TID 201). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 7 non-empty blocks including 7 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 192.0 in stage 2.0 (TID 210, localhost, executor driver, partition 192, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 174.0 in stage 2.0 (TID 201) in 67 ms on localhost (executor driver) (186/200)\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000166_195' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000166\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000166_195: Committed\n",
      "19/11/16 09:43:30 INFO Executor: Running task 192.0 in stage 2.0 (TID 210)\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 166.0 in stage 2.0 (TID 195). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 194.0 in stage 2.0 (TID 211, localhost, executor driver, partition 194, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 166.0 in stage 2.0 (TID 195) in 90 ms on localhost (executor driver) (187/200)\n",
      "19/11/16 09:43:30 INFO Executor: Running task 194.0 in stage 2.0 (TID 211)\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000172_199' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000172\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000172_199: Committed\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 172.0 in stage 2.0 (TID 199). 3777 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 5 non-empty blocks including 5 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Starting task 197.0 in stage 2.0 (TID 212, localhost, executor driver, partition 197, ANY, 7767 bytes)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 172.0 in stage 2.0 (TID 199) in 84 ms on localhost (executor driver) (188/200)\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 5 non-empty blocks including 5 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO Executor: Running task 197.0 in stage 2.0 (TID 212)\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 5 non-empty blocks including 5 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 5 non-empty blocks including 5 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 6 non-empty blocks including 6 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000176_202' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000176\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000176_202: Committed\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000173_200' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000173\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000173_200: Committed\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000181_203' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000181\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000181_203: Committed\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 176.0 in stage 2.0 (TID 202). 3863 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 181.0 in stage 2.0 (TID 203). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 173.0 in stage 2.0 (TID 200). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 176.0 in stage 2.0 (TID 202) in 131 ms on localhost (executor driver) (189/200)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 181.0 in stage 2.0 (TID 203) in 131 ms on localhost (executor driver) (190/200)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 173.0 in stage 2.0 (TID 200) in 135 ms on localhost (executor driver) (191/200)\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\n",
      "19/11/16 09:43:30 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "19/11/16 09:43:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000185_206' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000185\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000185_206: Committed\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000188_208' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000188\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000188_208: Committed\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000184_205' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000184\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000184_205: Committed\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000183_204' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000183\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000183_204: Committed\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 184.0 in stage 2.0 (TID 205). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 188.0 in stage 2.0 (TID 208). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 185.0 in stage 2.0 (TID 206). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 183.0 in stage 2.0 (TID 204). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000191_209' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000191\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000191_209: Committed\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 185.0 in stage 2.0 (TID 206) in 101 ms on localhost (executor driver) (192/200)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 183.0 in stage 2.0 (TID 204) in 109 ms on localhost (executor driver) (193/200)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 184.0 in stage 2.0 (TID 205) in 103 ms on localhost (executor driver) (194/200)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 188.0 in stage 2.0 (TID 208) in 96 ms on localhost (executor driver) (195/200)\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 191.0 in stage 2.0 (TID 209). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 191.0 in stage 2.0 (TID 209) in 95 ms on localhost (executor driver) (196/200)\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000187_207' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000187\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000187_207: Committed\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 187.0 in stage 2.0 (TID 207). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 187.0 in stage 2.0 (TID 207) in 99 ms on localhost (executor driver) (197/200)\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000194_211' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000194\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000194_211: Committed\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000192_210' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000192\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000192_210: Committed\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 194.0 in stage 2.0 (TID 211). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 192.0 in stage 2.0 (TID 210). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 194.0 in stage 2.0 (TID 211) in 104 ms on localhost (executor driver) (198/200)\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 192.0 in stage 2.0 (TID 210) in 108 ms on localhost (executor driver) (199/200)\n",
      "19/11/16 09:43:30 INFO FileOutputCommitter: Saved output of task 'attempt_20191116094327_0002_m_000197_212' to file:/Users/juanluisgarcialopez/repos-datascience/big-data-spark/out.json/_temporary/0/task_20191116094327_0002_m_000197\n",
      "19/11/16 09:43:30 INFO SparkHadoopMapRedUtil: attempt_20191116094327_0002_m_000197_212: Committed\n",
      "19/11/16 09:43:30 INFO Executor: Finished task 197.0 in stage 2.0 (TID 212). 3820 bytes result sent to driver\n",
      "19/11/16 09:43:30 INFO TaskSetManager: Finished task 197.0 in stage 2.0 (TID 212) in 97 ms on localhost (executor driver) (200/200)\n",
      "19/11/16 09:43:30 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "19/11/16 09:43:30 INFO DAGScheduler: ResultStage 2 (json at NativeMethodAccessorImpl.java:0) finished in 1.526 s\n",
      "19/11/16 09:43:30 INFO DAGScheduler: Job 1 finished: json at NativeMethodAccessorImpl.java:0, took 2.426356 s\n",
      "19/11/16 09:43:30 INFO FileFormatWriter: Write Job 122c47cf-5679-4adc-b340-bd585416216d committed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/11/16 09:43:30 INFO FileFormatWriter: Finished processing stats for write job 122c47cf-5679-4adc-b340-bd585416216d.\n",
      "19/11/16 09:43:30 INFO SparkUI: Stopped Spark web UI at http://juans-mbp:4043\n",
      "19/11/16 09:43:30 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "19/11/16 09:43:30 INFO MemoryStore: MemoryStore cleared\n",
      "19/11/16 09:43:30 INFO BlockManager: BlockManager stopped\n",
      "19/11/16 09:43:30 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "19/11/16 09:43:30 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "19/11/16 09:43:30 INFO SparkContext: Successfully stopped SparkContext\n",
      "19/11/16 09:43:30 INFO ShutdownHookManager: Shutdown hook called\n",
      "19/11/16 09:43:30 INFO ShutdownHookManager: Deleting directory /private/var/folders/k4/33yt9vyn3_x0v6tsv21x51_w0000gn/T/spark-ee3b1700-274c-456f-bf68-557b62232ac3/pyspark-44243b4d-c7dc-428a-a1f8-9fda8840e5eb\n",
      "19/11/16 09:43:30 INFO ShutdownHookManager: Deleting directory /private/var/folders/k4/33yt9vyn3_x0v6tsv21x51_w0000gn/T/spark-d0877c2b-ca1a-40e4-852a-fab3a635a7d8\n",
      "19/11/16 09:43:30 INFO ShutdownHookManager: Deleting directory /private/var/folders/k4/33yt9vyn3_x0v6tsv21x51_w0000gn/T/spark-ee3b1700-274c-456f-bf68-557b62232ac3\n"
     ]
    }
   ],
   "source": [
    "! unset PYSPARK_DRIVER_PYTHON; spark-submit sum_amount.py data/coupon150720.csv out.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Running on cluster versus client mode\n",
    "\n",
    "This setting controls where the driver runs.\n",
    "\n",
    "The default deployment mode is `client`, that is, the driver runs on the machine that is running the spark-submit script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "```shell\n",
    "./bin/spark-submit \\\n",
    "    mysparkjob.py \\\n",
    "    data/coupon150720.csv \\\n",
    "    --deploy-mode client\n",
    "```\n",
    "\n",
    "\n",
    "```shell\n",
    "./bin/spark-submit \\\n",
    "    mysparkjob.py \\\n",
    "    data/coupon150720.csv \\\n",
    "    --deploy-mode cluster\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "\n",
    "\n",
    "[hadoop fs](https://hadoop.apache.org/docs/r2.8.3/hadoop-project-dist/hadoop-common/FileSystemShell.html)\n",
    "\n",
    "[standalone Spark versus yarn versus Mesos](http://www.agildata.com/apache-spark-cluster-managers-yarn-mesos-or-standalone/)\n",
    "\n",
    "[How Spark runs on clusters](https://spark.apache.org/docs/2.2.0/cluster-overview.html)\n",
    "\n",
    "[spark-submit](https://aws.amazon.com/es/blogs/big-data/submitting-user-applications-with-spark-submit/)\n",
    "\n",
    "[Cluster versus Client deployment modes](https://stackoverflow.com/questions/28807490/what-conditions-should-cluster-deploy-mode-be-used-instead-of-client)\n",
    "\n",
    "[Tunnelling web connections through ssh to view the Spark management web views](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-ssh-tunnel.html)\n",
    "\n",
    "[Findings on running Google Dataproc](https://www.inovex.de/blog/findings-in-running-google-dataproc/)\n",
    "\n",
    "[Dataproc - Spark cluster in minutes](https://medium.com/google-cloud/dataproc-spark-cluster-on-gcp-in-minutes-3843b8d8c5f8)\n",
    "\n",
    "[Using the `gcloud` command line tool](https://cloud.google.com/sdk/docs/quickstart-debian-ubuntu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create cluster\n",
    "- Upload job.py file. \n",
    "- Submit job. \n",
    "- ???\n",
    "- Profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
